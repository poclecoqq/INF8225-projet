{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaRrf9VjyBCw"
      },
      "source": [
        "# Main notebook\n",
        "\n",
        "- DCGAN Adapted from [this tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/)\n",
        "- VAE adapted from this [tutorial](https://keras.io/examples/generative/vae/#train-the-vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCLBll_NyBC1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCwF4rBVyBC2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def is_in_collab_env():\n",
        "    return 'google.colab' in sys.modules\n",
        "\n",
        "if is_in_collab_env():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT = Path(\"/content/drive/MyDrive/INF8225/notebooks\")\n",
        "else:\n",
        "    ROOT = Path('..').resolve()"
      ],
      "metadata": {
        "id": "hNPxBlC227lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8ec591-34f4-48f7-acc3-baebe6896c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DATA_PATH = ROOT/\"data\"\n",
        "ROOT_MODEL_PATH = ROOT/\"models\"\n",
        "\n",
        "ROOT_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
        "ROOT_MODEL_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "cUnqb5rI2-rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPUbJUhByBC4"
      },
      "source": [
        "## Prepare CelebA data\n",
        "\n",
        "We'll use face images from the CelebA dataset, resized to 64x64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9VZevZayBC4"
      },
      "outputs": [],
      "source": [
        "def get_celebA_dataset():\n",
        "    ## Download dataset\n",
        "    os.makedirs(\"celeba_gan\", exist_ok=True)\n",
        "    url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "    output = \"celeba_gan/data.zip\"\n",
        "    gdown.download(url, output, quiet=True)\n",
        "    with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "        zipobj.extractall(\"celeba_gan\")\n",
        "\n",
        "    ## Build keras dataset object\n",
        "    dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "        \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        "    )\n",
        "    ## Preprocessing\n",
        "    dataset = dataset.map(lambda x: x / 255.0)\n",
        "    return  dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "\n",
        "    ds_size = tf.data.experimental.cardinality(ds).numpy()\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    # TODO: deterministic splits\n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "dQdLVN4GNB6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LIsh-79yBC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8341114-88a4-4a66-f421-32ac6cee6364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "dataset = get_celebA_dataset()\n",
        "## TODO: adapt code with train test\n",
        "train, valid, test = get_dataset_partitions_tf(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0zHMM3jyBC7"
      },
      "source": [
        "Let's display a sample image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b1RqfjdyBC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "e658acaa-19b0-4900-c5ff-095ab10a48ba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architectures"
      ],
      "metadata": {
        "id": "pOeojm615FW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DCGAN"
      ],
      "metadata": {
        "id": "o1fAbtpW5PID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.discriminator = self.__get_discriminator()\n",
        "        self.generator = self.__get_generator()\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    def get_generator(self):\n",
        "        return self.generator\n",
        "\n",
        "    def __get_discriminator(self):\n",
        "        return keras.Sequential(\n",
        "                [\n",
        "                    keras.Input(shape=(64, 64, 3)),\n",
        "                    layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Flatten(),\n",
        "                    layers.Dropout(0.2),\n",
        "                    layers.Dense(1, activation=\"sigmoid\"),\n",
        "                ],\n",
        "                name=\"discriminator\",\n",
        "            )\n",
        "    def __get_generator(self):\n",
        "        return keras.Sequential(\n",
        "            [\n",
        "                keras.Input(shape=(self.latent_dim,)),\n",
        "                layers.Dense(8 * 8 * 128),\n",
        "                layers.Reshape((8, 8, 128)),\n",
        "                layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "            ],\n",
        "            name=\"generator\",\n",
        "        )\n",
        "    \n",
        "    def generate_samples(self, num_img):\n",
        "        \"\"\"\n",
        "        Generates num_img images.\n",
        "            Returns:\n",
        "                imgs: an array of PIL images\n",
        "        \"\"\"\n",
        "        random_latent_vectors = tf.random.normal(shape=(num_img, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imgs = [ keras.preprocessing.image.array_to_img(generated_image) for generated_image in generated_images]\n",
        "        return imgs\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "u2mfR4Hk5cWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE\n"
      ],
      "metadata": {
        "id": "JYsq8qxt7y0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "u2Hp2RCkK6cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, latent_dim, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = self.__get_encoder()\n",
        "        self.decoder = self.__get_decoder()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "    \n",
        "    def get_generator(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def __get_encoder(self):\n",
        "        encoder_inputs = keras.Input(shape=(64, 64, 3)) # avant 28, 28, 1\n",
        "        x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(self.latent_dim, activation=\"relu\")(x)\n",
        "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
        "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def __get_decoder(self):\n",
        "        latent_inputs = keras.Input(shape=(self.latent_dim,))\n",
        "        x = layers.Dense(16 * 16 * 64, activation=\"relu\")(latent_inputs) # avant: 7 * 7 * 64\n",
        "        x = layers.Reshape((16, 16, 64))(x) # avant: 7,7,64\n",
        "        x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(rate=0.25)(x)\n",
        "        x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(rate=0.25)(x)\n",
        "        decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "        return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "    \n",
        "    def predict(self, input):\n",
        "        z_mean, z_log, z = self.encoder.predict(input)\n",
        "        return self.decoder.predict(z)"
      ],
      "metadata": {
        "id": "BeisHTws72i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeOQ2YkyBDC"
      },
      "source": [
        "## Training code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "d1t400Il7-e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejwQuN9HyBDB"
      },
      "outputs": [],
      "source": [
        "class SaveImagesCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, dir_path, num_img=3, latent_dim=128, epoch_offset=0):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.epoch_offset = epoch_offset\n",
        "        self.dir_path = dir_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch_adjusted = self.epoch_offset+epoch\n",
        "        if (epoch_adjusted % 10) == 0:\n",
        "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "            generator = self.model.get_generator()\n",
        "            generated_images = generator(random_latent_vectors)\n",
        "            generated_images *= 255\n",
        "            generated_images.numpy()\n",
        "            \n",
        "            dir_path = self.dir_path/f'e:{epoch_adjusted}'\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "            crnt_timestamp = str(time.time()).split('.')[0]\n",
        "            for i in range(self.num_img):\n",
        "                img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "                img.save(dir_path/f'generated_img_e:{epoch_adjusted}_{i}_t:{crnt_timestamp}.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "zFJdp0zi8Ah9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "class Dataset_names(str, Enum):\n",
        "    CelebA = \"CelebA\"\n",
        "    Anime_faces = 'Anime_faces'\n",
        "\n",
        "class Model_names(str, Enum):\n",
        "    GAN = \"GAN\"\n",
        "    VAE = 'VAE'\n",
        "\n",
        "def get_model_dir(dataset_name: Dataset_names, model_name: Model_names):\n",
        "    path = ROOT_MODEL_PATH/f'{model_name}-{dataset_name}'\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def get_data_dir(dataset_name: Dataset_names, model_name: Model_names):\n",
        "    path = ROOT_DATA_PATH/f'{model_name}-{dataset_name}'\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "iSsKnXFMXWXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name=Dataset_names.CelebA\n",
        "model_name=Model_names.VAE\n",
        "MODEL_PATH = get_model_dir(dataset_name, model_name)\n",
        "DATA_PATH = get_data_dir(dataset_name, model_name)\n",
        "\n",
        "## Model selection\n",
        "use_gan = False\n",
        "\n",
        "## Model configuration\n",
        "load_weights = False\n",
        "weights_path = None\n",
        "latent_dim = 128\n",
        "\n",
        "## Training specifics\n",
        "epochs = 30 # In practice, use ~100 epochs"
      ],
      "metadata": {
        "id": "tuJ0XRezBTqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_GAN(latent_dim, load_weights= False, weights_path=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        latent_dim: The size of the latent dim\n",
        "        load_weights: When set to true, loads weights\n",
        "        weights_path: The path to the weights\n",
        "    Returns:\n",
        "        gan\n",
        "    \"\"\"\n",
        "    gan = GAN(latent_dim)\n",
        "    gan.compile(\n",
        "        d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss_fn=keras.losses.BinaryCrossentropy(),\n",
        "    )\n",
        "    if load_weights:\n",
        "        gan.load_weights(weights_path)\n",
        "    return gan\n",
        "\n",
        "def get_VAE(latent_dim, load_weights= False, weights_path=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        latent_dim: The size of the latent dim\n",
        "        load_weights: When set to true, loads weights\n",
        "        weights_path: The path to the weights\n",
        "    Returns:\n",
        "        vae\n",
        "    \"\"\"\n",
        "    R_LOSS_FACTOR = 10000\n",
        "    vae = VAE(latent_dim)\n",
        "    vae.compile(keras.optimizers.Adam(learning_rate=0.0005), R_LOSS_FACTOR, metrics=[\"val_loss\"])\n",
        "    if load_weights:\n",
        "        vae.load_weights(weights_path)\n",
        "    return vae"
      ],
      "metadata": {
        "id": "sM46ZeXR-FTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "if model_name == Model_names.GAN:\n",
        "    model = get_GAN(latent_dim, load_weights, weights_path)\n",
        "else:\n",
        "    model = get_VAE(latent_dim, load_weights, weights_path)"
      ],
      "metadata": {
        "id": "xl0sehedAMtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_images_callback = SaveImagesCallback(dir_path=DATA_PATH, num_img=10, latent_dim=latent_dim)\n",
        "mcp_save = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH/'e:{epoch:02d}',period=10, save_weights_only=True)\n",
        "\n",
        "model.fit(\n",
        "    dataset, epochs=epochs, callbacks=[save_images_callback, mcp_save]\n",
        ")"
      ],
      "metadata": {
        "id": "z4Op5ie79OU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z67zpcrgyBDC"
      },
      "source": [
        "Some of the last generated images around epoch 30\n",
        "(results keep improving after that):\n",
        "\n",
        "![results](https://i.imgur.com/h5MtQZ7l.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "32_LvJVcZqcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Model selection\n",
        "use_gan = False\n",
        "\n",
        "load_weights=True\n",
        "weights_path=MODEL_PATH/'e:01'"
      ],
      "metadata": {
        "id": "pWGmyfGSCIdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "if model_name == Model_names.GAN:\n",
        "    model = get_GAN(latent_dim, load_weights, weights_path)\n",
        "else:\n",
        "    model = get_VAE(latent_dim, load_weights, weights_path)"
      ],
      "metadata": {
        "id": "pkDBjg_PZ--v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating images"
      ],
      "metadata": {
        "id": "65FPnXLZfSYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_samples = 100\n",
        "\n",
        "# # Generate samples\n",
        "# imgs = gan.generate_samples(n_samples)\n",
        "\n",
        "# # Initialize directory for pictures\n",
        "# dir_name = f'DCGAN_CelebA-generated'\n",
        "# img_path = ROOT_DATA_PATH/dir_name\n",
        "# img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# # Save images\n",
        "# for i, img in enumerate(imgs):\n",
        "#     img.save(img_path/f'generated_img_{i}.png')"
      ],
      "metadata": {
        "id": "lRcxXYZtbzxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving random images **TODO fix with test**"
      ],
      "metadata": {
        "id": "biMA_U42f7ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "train, _, test = get_dataset_partitions_tf(dataset, size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjBc-WQ1f7y3",
        "outputId": "9b762abf-5f9b-42d0-c797-9ffcb0fedc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_name = f'DCGAN_CelebA-real'\n",
        "\n",
        "real_img_path = ROOT_DATA_PATH/'DCGAN_CelebA-real'\n",
        "real_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "gen_img_path = ROOT_DATA_PATH/'DCGAN_CelebA-gen'\n",
        "gen_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "i = 0\n",
        "for image_batch in test.as_numpy_iterator():\n",
        "    for j in range(image_batch.shape[0]): # batch of 32\n",
        "        # Generating img + saving it\n",
        "        img_gen = gan.generate_samples(1)[0]\n",
        "        img_gen.save(gen_img_path/f'{i}.png')\n",
        "\n",
        "        # Saving real image\n",
        "        img_real = image_batch[j,...]\n",
        "        plt.imsave(real_img_path/f'{i}.png', img_real / 255)\n",
        "        i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "7HfX2OlfhHGu",
        "outputId": "3d4dd567-c44d-42c9-a104-72d2f31cc3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-d319110a1beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Generating img + saving it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimg_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_img_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{i}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9d5dd8728877>\u001b[0m in \u001b[0;36mgenerate_samples\u001b[0;34m(self, num_img)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FID"
      ],
      "metadata": {
        "id": "jMTi8OaIZ2Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yBK1xSWZscB",
        "outputId": "3892be38-fbb0-4e0c-d09b-8b8e0914299b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch-fid-0.2.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.21.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->pytorch-fid) (4.1.1)\n",
            "Building wheels for collected packages: pytorch-fid\n",
            "  Building wheel for pytorch-fid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fid: filename=pytorch_fid-0.2.1-py3-none-any.whl size=14835 sha256=5090abf0d28a2ecc5e60f0bebd08b90fda9e408801d191841ef0be954cba2519\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/ac/03/c5634775c8a64f702343ef5923278f8d3bb8c651debc4a6890\n",
            "Successfully built pytorch-fid\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid $real_img_path $gen_img_path --device cuda:0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p5-aPZEZwOw",
        "outputId": "da2dcfcc-aa3e-4a47-d3d8-daac0a9eb839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:05<00:00, 17.2MB/s]\n",
            "100% 719/719 [02:53<00:00,  4.14it/s]\n",
            "100% 333/333 [01:12<00:00,  4.60it/s]\n",
            "FID:  51.430461225018604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FID:  51.430461225018604"
      ],
      "metadata": {
        "id": "4_SJy8c9ADtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GEI8C9h-ZwRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specificity"
      ],
      "metadata": {
        "id": "CAlwv1T1Z3-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_img(reference_img, dataset, dist_func=None):\n",
        "    \"\"\"\n",
        "    code below finds closest image from reference_img\n",
        "    takes 2 mins for a single img ...\n",
        "    \"\"\"\n",
        "    closest_dist = 1e9\n",
        "    closest_img = None\n",
        "    for image_batch in dataset.as_numpy_iterator():\n",
        "        \n",
        "        truth = image_batch\n",
        "        distances = [tf.norm(truth[i]-reference_img) for i in range(32)]\n",
        "        i = tf.math.argmin(distances).numpy()\n",
        "        min_dist = distances[i]\n",
        "\n",
        "        if closest_dist > min_dist:\n",
        "            closest_dist = min_dist\n",
        "            closest_img = truth[i]\n",
        "    return closest_img"
      ],
      "metadata": {
        "id": "oKJHxoE6Z5sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "min_dist = []\n",
        "n_samples = 10\n",
        "imgs = model.generate_samples(n_samples)\n",
        "for img in imgs:\n",
        "    closest_img = find_closest_img(img, train)\n",
        "    min_dist.append(tf.norm(img-closest_img))\n",
        "print(np.mean(np.array(min_dist)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiqXMPgvhdWS",
        "outputId": "a9bb479e-944b-4580-d265-2798949df9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5223.487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5223.487"
      ],
      "metadata": {
        "id": "RWjXGDsJVzVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "1uCD5-iHhdpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb\n"
      ],
      "metadata": {
        "id": "7JJbZw1ghd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3e9TJNOhgar",
        "outputId": "d8764aae-1e0d-4f06-8601-fcf2dff5b89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8WHoN2O9hi-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oCLBll_NyBC1",
        "sPUbJUhByBC4",
        "pOeojm615FW9",
        "o1fAbtpW5PID",
        "JYsq8qxt7y0h"
      ],
      "name": "main",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}