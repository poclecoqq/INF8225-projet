{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaRrf9VjyBCw"
      },
      "source": [
        "# Main notebook\n",
        "\n",
        "- DCGAN Adapted from [this tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/)\n",
        "- VAE adapted from this [tutorial](https://keras.io/examples/generative/vae/#train-the-vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCLBll_NyBC1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LCwF4rBVyBC2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def is_in_collab_env():\n",
        "    return 'google.colab' in sys.modules\n",
        "\n",
        "if is_in_collab_env():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT = Path(\"/content/drive/MyDrive/INF8225/notebooks\")\n",
        "else:\n",
        "    ROOT = Path('..').resolve()"
      ],
      "metadata": {
        "id": "hNPxBlC227lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5946b7-ccbe-445a-9e9c-d34d8c4241ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = ROOT/\"data\"\n",
        "MODEL_PATH = ROOT/\"models\"\n",
        "\n",
        "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "cUnqb5rI2-rE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPUbJUhByBC4"
      },
      "source": [
        "## Prepare CelebA data\n",
        "\n",
        "We'll use face images from the CelebA dataset, resized to 64x64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T9VZevZayBC4"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"celeba_gan\", exist_ok=True)\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "\n",
        "    ds_size = tf.data.experimental.cardinality(ds).numpy()\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "dQdLVN4GNB6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-LIsh-79yBC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7c6a36-a046-4fac-d019-67e5ec3ace6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)\n",
        "train, valid, test = get_dataset_partitions_tf(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0zHMM3jyBC7"
      },
      "source": [
        "Let's display a sample image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9b1RqfjdyBC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "c9ce1d0f-37aa-4f19-a6b2-8a7a078ed1d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO19aZBk2VndW/LlUpmVWfvSVdVV1Xv3zPTsmtEyGiSNJARCGNkgCBtj44DAxkBgB4Ed4S0c3ohwgP2HwCGDArAtYxgLS0LSCGkYjaTZ957u6eq1urrWrqqsLfe3+Qf2+875uipnbBOhN457ft3qe/PlW/L2O99yvs+O49gyMDBIH5zv9QkYGBjsD7M5DQxSCrM5DQxSCrM5DQxSCrM5DQxSiky3ycefX0hcubaaK2S8ZOzGLZob6ssn45GBUjIeLPDXeZ4cw7b5GxznoP832LscxxGM9VnKMTrtIBkHQUCrOhGMA/+A77WsMAzVd8f7joPIOnCdbfP5499hJMdvt9u8LpJri7SHPZbrdGxXPuO4tAxPK+JLsVpNue7dZicZL9f5ftSdghzf53PMwO3XzxOBc1HENyubkWvBGf1oQ/gd6LlcNpuMyyH/jiolufDxAbk/xSzfkJ2d7WR8ee4aza2uLidjx4FnYfO1tFtw72J+FkEke+GXf+kX9r1Z5s1pYJBSmM1pYJBSdKW1oSWvYlcR2xj2dWxr+iRz4cHs5v8zwIVqnoUs1O6S9EE0mf/fDB05Zhyp49NXI63lY0RwXredoiMULLTx+fHCCA7JT/0vBkRl3+Fn9C3lv/X7ByaRat52Qzz44y/+R+xY0TtYY2BgkEqYzWlgkFKYzWlgkFJ0tTmtWNzLtuLdrit8PW6zuz2Gw7pA8XUoIpORdTp0ol3seHREF4/9bWvlM8qOgriCFgLgWv05WkvhEufgdbfZL/uHY6I4OmDV7VdF32cfbMvwOp7D8ExEcRY+nu0cfD/sdxhKQehrCcFg7GZzOnTr1TML4W+Hw2b2AXdSPzMbfsOO7VkHAT8XxaGaAxtf+2zegeDEvDkNDFIKszkNDFKKrrQ2EzSScSGbp7mCK3Qn28Ov/SJwDs+SV73nFWjdwVlAB9Oi26klZgjp7CEZI6XWGUJRhJRO0bgu2Sy0DuieowiZA9zecfn8MUvKdeUeV6wePkdfzvn2c4SQF4xDxZxC+AdbBUIqxVwyLhQk86e5sUPrmo36gcf4iwg5BHBtTheTAuGo743g+WaKKkOor5iMszn5rnqdr/PylblkvLq6QnNo0nmeHL/V7tA6fE46ey2wDqbK/xvmzWlgkFKYzWlgkFKYzWlgkFJ0tTlHypLdX+kt0VxvXuxHz2UbKJ8T26YXbJmMwzybXM3KXtRhF/kM/432ov4M2liBlorgugjsuVC53jFtTqeJgRvdQZtTmWJ4yl6G/z/MZuXvDNgvnD5mWdmCPIu4S+YXXrO26UNUtkR8MSQwseW7hkN+7nsdUWt0OvpE/t8T+siehvPPqGuxMXQV8TMr94q9Pj5apLkC2JnbW6vJ+NxrL9O6Zl3sR23vop+g02km41D5AtB+jpQqxX8H70Xz5jQwSCnM5jQwSCm60tozk33JOJ/nUEoWBK3dgNTqnWaNaCBdDUOmDt1oLbuyZZ3vq4wmcHPbOgwCVC1S8gd0qdNnFNtDymsrl7oNjCyGa8nm+NFg2MLu8l9qBsXKOuQCp++HfA8w3FOA7+7N52hdJSc0t9phkf07VZGQ+FzNZZDVupClo2yKvCd/j4/00txAST7n2Xs0t3ZDwiKvvvKSnIe6H6US0GH1zFAI3wogRKfedUFG7lUQqtCJ/fYmgHlzGhikFGZzGhikFF1pbaVSScba83dQ/RyNbh7Zg46n/8aMniDQXtyDM3gOorKa/gYWUnSmG66DgnNFa4FfEuONtWcRaK32koLr1QFOF/tKJPAOxLmWdTDVtizLwq8OVRGhEM6j1ZG5erNJ69q+eDFvK9n0fwOtcQa6F4ALuV9loU0OC5Udq7DJFXZ2k/H8jes0d2lOMn/QNHNi3grtDvzm1O/Fh0eBovjIZlPPjiFSoee6ie7/9zm97QoDA4PvCczmNDBIKczmNDBIKbranN3CD2iDvlN7sZs91O27uwuerQPn8O9cTvh/U9lRIdg2rjqG64qtkMvy+dux2F8FCDm0QblhWZbVAUWJr0JBNtyTTFbsKt9XdU7hvLralWB36/uBplOo7OcAxdZYMEwdP7S6qYD2z07qqj7SIuRQwjN9PfLznB7hTJ+RPplr7azR3NzF88n45uIizaHAH/0JLZVBhr/bUBnXEWybGNQltlKaOI78dnQI0ApZwbIfzJvTwCClMJvTwCCl6Epr8dXejcJ0q7uDOCiZfT8cRGW71fHpFu5ptYQuabG1GwvN7cmxy7sISfx+q0FzW+uSbXJ1cyMZF/IqiR+KvQ6NTtLcwPBUMt5tyHm0VFK51yO0Tt8DvDak7/p+BAFS14Nr39qufM7NqkwlG80Zvo8H/Sa6PXf92xkty/dNjfQn40qO19U2JWn9zTffpLnNzXU5f1VTCTPA0FTzA03zZRypd1hsyznaMHZV1o8N6V+2xa0rso4JpRgYvGthNqeBQUphNqeBQUrxNqEUHGu3/MHpZKTCoFqpB9ec7VqjNCNcPpthXu+BazzocIikUxdFQjYUe9H22XbswN+1PVZaNMFm2alu0JwViR3hBOIaz8X9tKzRrCXjF7/1VZpzM5J65oAYfeb4KVrXMyi2qlNkFUYAdYMbuzIeHBqhdT4IsUPdHhAfNrSns9mstHrgc42MLpQG6YwQVshYqp6rJc9peJjF3LMDIuLPQM3ZW7du0rrz599Ixs2mapdIxdZUiMSX5+tDI58oZl9DEIntHrm8TTJkc2IPGw6PoM2Zz/AvPGvE1gYG716YzWlgkFL8H9Dad5YN8uc4QK6g/tkBl72jFMTo5u5W5h/d+XbMWUx5SNjo7IhSwa9v07pmU2htTomcl1eWknGoaHOxR6hPbVeOubJUpXXttmQM7e5t0lwOMosG+4eT8dybL9K6wL2QjCcnp2nu4tzlZHzi1JlkvLm8QOtu7Qi9PnLmDprrLQkVr0Nn61abqWsbMmkCrdJBcTSYPQWPn9nwQDkZD/apUA1kzszfuJqML17kcIkfCJXNeqxKiSh0c3AdpTDGGr8sKs9k4G9HbRNbju8AZXcdRd8hgyyX4WMU3r5srXlzGhikFWZzGhikFO848b0bre0OoL+KBmHZ/ECXFYQ5pLKeTj5Hb1zEtBaL+YTgka2uc3l9C6jJ1Tn2ChbyWJKSz3FvWyjq6KhQ0vHJQ7Tu1rpQ48Ie33JsJ/HWJaGuy8t8joP9Q8l47vXnaO6Tn/qRZLy1JZ9b3uIWA/ke8fK+9I15mps5KXS4Mn4sGevaNz4w1GKBqWDQEqpZgMucGKrQumGgss36LZq7PC+J6gs3RSjdrTyl76u6UiF6jfl++yCqDrEbXoa9tSTsUMJ07L7nweFzHr/r8nBMW7m923WOCuwH8+Y0MEgpzOY0MEgpzOY0MEgp7G62Y6cjuflayXGQGNqydK1asDmVTdhoiB2oj9/piBt6cHAwGbsq0yLsSJjCb3KN0tWb1+T4TQmlRCokslOV0EdH1WKdnJLMHLRzLMuyLMhE6UDoZ35hmc9jGeyoxWs0NzomWTzLi2Iv2ur/TRvs3fXNLZpzXTmv2WMnk/HYKNu+a6siSvY7fL+v3xTb7/s+9ePJuG/mLlrXzkktYy2sIAUICNh7C3wtLoRLLs69QXOrq6I26YFQVayyjAKwM/2OKprmw98uh1kCR45pQ0tKXRfXjeV34Fl8rzIZEPFDO42c9uCAcLzdVAL8hnzfz//9X9039mjenAYGKYXZnAYGKUXXUIr9DuvW6AwhboMg41qdaSfWc8Hkbctiuodi3dqu6rRcF0oatGo0F0EWSW13C9YxxSgBNfnWc8/T3OYtCGm4fJ29UNcX05iyDnfwzgK1Hyhyx2oMRvT3yJzrcZgCW0E0O5zojbVqFm9KVhDWLrIsy5o9NJ6ML507R3N2R57N8hWhmm0llB49LjR3sDJAcx5kCDWaQum2t9dp3daWPLOtHRYhFApCQ0Ogv5FqrYbd1OJYi5zlOQWKrkaY8A/jOGJzJmtjuIQT2rOQ8eRC2lurzs8lANMsUnWIXU+F/faBeXMaGKQUZnMaGKQUZnMaGKQUXW1OB4oSxSpcgp15Y9VZ2AW3d21PbL1iRtX19KRo1eDwIM21IaSRcYCf+xwG2YI0t+rGEs1lwY6oV2Vuef4irQvA9p0+cZLmhg+JesPJce3UvgGxES9fejUZ55X8Zmx0NBmvr/M5Xr8moZXh4bFkPDU5ResgY8wqOEM0N3dJ7MxCSezPVmOX1i2tiY11/8OP0NwLL34zGV8690QyHtzgXiODRXmG+f6Haa4D4bDVFQkfNZt8HjFcTD7LtmQHhOkxiKH9jipWFsl5xBbb5zHWlXVV2h+ERbxIzivrqN+wK38XPN4mDihbOi0Qujd0S0T521V2ayH79u9F8+Y0MEgpzOY0MEgputJa6PZmRUoMjXVPtVC6VpNwR7koNWIyqht2BHWJopDpqgOUY3MFqFXI7mrXBqG0w7QiaMt5LC5eScaxyvhohnIbJo5w7Z7B0dlkPHb4CM1duSwC4O/7yMeT8Rd+/3dp3aU5UZvMHpmluWyPiJyXFyXksLx2ida99pqEPtZucQbST/30X0/GP/DJTybj6haHlj7/X/5TMv7GN79Jc1gX5/RJuQfL17mdwQvOl5NxZXiY5m4sS5ZRFcIlrqr75IPKo9nmZxZFsBZCJIGqeYRCaVuJoV3421F01YbQCpbkLej6vFhSKeYwSKMlv1W/LZvEUWJ/D7qFZ1W40dQQMjB4F8NsTgODlKIrrW1Dh+NIlRjMQBK4qkJv9UGmiwV1VJoNTtjuAKUpZJg61DaEui2DJ3RXZQghVR7p4+ybG0tCyVzI0h4Y55YIw0fOJuOJ6aM01z82k4y/8IU/orn3v/c9yfibX38yGR+eZepqQ6L0yZOnae7yJfHWrr0uNXOuXGbRtw33+z987jdpbnBIMpIWb95Ixv/uN5heezk5j7/2t36a5t58Vb779ZdeS8YzU0xdV+elM/TTT/D9mJwWkXYZnoud4+Tz7Zo8a1+1nWhR1y75YTm28vSDhz1S3cItoJdZJXLG7J4c0E69ETodMZc09Q5C8PhCsn9BtYzIgqM4o0/xtg7tt8O8OQ0MUgqzOQ0MUgqzOQ0MUoquNmcHWgzk82w32JC17yohrANtCvym2Ii7G2xHPf/c08l4+cZlmstDVpDTlpBANs9ZOoWS/D33AtdpHR0WZcvMYbEDryxyUSlvW1QqR1QW01tvvJyMH32EM2IKILY+fkTCLJfffI3WnT4udua511lc/OSTTyXjvR1x0Zd7+NH87M/91WR8eJRbGHzxS48n405T7ttEhQ2dwRFRpTz37e/S3Je/Iudxz113J+PFFe4aPVAWu/78C1xobGxAVCpOXhQ79aYSK3tSt9ZTaqcQbEuqjazCdQ6oVDIuX2cGwhZZpf7AORtqHjdrHMprQ/dzXdgtj3YrGJbavsVzDJVCqO0bm9PA4F0LszkNDFKKrrQ2lwNXtvIFO+BC9luc2FyvSabL89+WEMOVV5gGnTwlYYu7T87QXMmT795aleT21U1uZ7ANHaUPjY3RXA8kqhcLQqUmJ7hL18y0tDd48Xmme2MTEnbJ2UxFnn5SsmwGy3L8cp5DOnPnJJPouW8/xecI93V1Ryjk0CB3Kis40u7huT/7Gs0tXpTwxisvCG3+zI/+MK37zgvn5TPbnGk1OiLn3DcEtXs6bEZsr4ko+557j9Pc3Dn57tGpE8m4NHKY1uUKQnkdVXhndRtaWUAmkW1peip0MqNE8J4n9zSXYzMlhLS3dkPMmVads6ls+O5ilhPr80DFXVvOq6NEGVhoAMXhlmVZ0UEtSwDmzWlgkFKYzWlgkFKYzWlgkFJ0tzkLYofEAacwebHw61urHAb54z/4XDIOIA3q6FkWMvf1i+2hhdhrkIb27EuvJOPxCbYrCyUJK/T0j9Pc7IzYPfPzEmZxVUGopRtisw1WyjRXBNvmpW89RXPTUISs3Ct2yeJVDj/srct3l7Nsexw7JiGYHIhzH3zwAVp346LYi7UatzD8vvffl4xjX+yoSBW3ysItvvsktxE8cvyjyXhnT57Z3KWrtO6u+2aS8QsvsH3+43/l08n44luiorFjtm99EOdXO2zPWZ74AzAU0aPaCHqgNrE9VfwLhNItVdO20xDbMgRhd0bZtAVIdcxmlBIKfj4hhETiQNmVkFYYqrrMYWhCKQYG71qYzWlgkFJ0bcfQbreTSbvDtV6rS0IFr775As11doXW3X33ncn49SvcimBvR+jZ6iJnD00BZYyhjUOtzmGbmSMzcoxVzvxxgLVnXKFPY+PcpsAuSosB22amP9gnqozlmyw8HhmTuQio2spNpoKNmpxzucg07hkIrQz0CUWfneHww9aeULAg4Ho067fkfvdBLdnnn3+J1g2DOLrt8zE2NiWEcc+9DybjkWE2I16HsNDpO+6hOWwhgbWMOw5fc8sV06Glavx2IHsI676WVIaamxEqG8VMGQMQ5DdUq73Il79zHtQyUqqoDHapVhQ0AvqKbUQilUmEf+t2I3jOv/xPfse0YzAweDfBbE4Dg5TCbE4Dg5SiayilWZfKBa0q11v1InG3lzy2W29BL49XX3k9Gbu93H78DShadedpDrOEwNd3d8Teeuh9rAxxwK99a4OrJAyDKsUHVUB1t0rrpsam4TOjNLcElQpGh7i2brMh4aQOuM2njnKRsABq8NZ3uG/Ive+R67nzlKTD6SJecY/cg1IPp9Qt3xLbffao3MdLysYPIMwyMcLXMtIv9u59ZySt8toVrlv7AKRcLt3i34QNbdYffFCua+7qDVo3XJZ3wq6uaQtVNBqULsk2cqclf7fa7A+xIqyEwOZcJrd/2h+m4VmWZcUR9DlRtmQALQZxTq/DcIm2OYOQr2c/mDengUFKYTangUFK0TWUsrb4ajKZizjjfmlO6Or2BocwNndl7f0PPpSMf/+3f4fWPfSQuOzHxrjFwPy8hCNGR4WeDg4wHduoCqUbVEqOKsz190u4ZGODqeUQZBKFbaYb/q4c4/nvPENzR06dScbjMyLmrlSYvreA/vYUOKzQA6GVDbiPccg0qwatDrM21//1oKbrrSUJSV27+hatC30xRTKa7gElHemXZ1EpcZs/C9QVDfV/+8q6KIQykI4Uq7qyHhT80iLqtUjWegV51s2As7p2doXm+yrMl80IhexRGVkuFgajkJTaB6BK0QXEwmD/EInO+sE5TWtDKEjwj37tD00oxcDg3QSzOQ0MUoqu3tqdLck8Ge5lAfGtTaF7k4c4ifrse6RDVhte52fvvZfWrQPtzBc4AyQLHcgOjcvx6g3uhNzfJ7Sr3WIqODQg9GxhUZLP9XcVe+TvuavcgcxqiAd4doZruE6MCn3Ngmh6Y429mOOH5PwzOb6Pbl5obd+QnEeryZ7nQlm8qTevcTbVmWMzyfjKnGRuaQo9eEho4vYWH//QuCTgjwyKwDyrOmxhttbWHtNJB7pxb1SFhuv7jdlJvRUWvkfw3G2goLtLLLLHzl85JbbOQCHljM0eVBu9q9hyITg4uycMlbcW6tb6gdyPKNTJ7UBrVUZWs8W/4/1g3pwGBimF2ZwGBimF2ZwGBilFV5tzakCyZS6ee53mGlXJetkIOTTx9S9+NRkfPy1hisoAhzqOHoNsE6X4OASFtRwIHUQRF2IqQShiZZltvdOnpV6sBwXDcgW+7M0VyaRpbq/S3Op1seGmDrFC440XJYvnvvd/MBm7AdtitR35vnbIYZDhQ6I+KQ+KTVi9zPd0e1vCFMdPnqG5jXXxDRyBe7q+wv/35sEubqlasgP9kBkFKhKlH7Z8CAG4SiCfzcm19faK7fjMsxyCGj0kobFCidsqDo9MJOPNqhQTOzTAtXrXwefhZjkSQRGNiOcOUpEEAfsrMCziq7CWD31OfB9szoBF2XhMX9mcLVW8YD+YN6eBQUphNqeBQUrRldbubov7ulVjqobZOAP9XHfnkQ8/moxjSF4eG+VQxMtQG6jT4df+UL+42PewU3aJ6c0C1BoaGOTjt9pCK/Z2JIm/mOVMjptXpQbSzWtcD2lyRMIxbsT3YLgiQuGNBQnBaNd7vSb0bOgw13rd2ZV7vLQqNLnSy/f07FlpkbB6kxPJXcjGaTaFxp248z5aV4UMpIFRppOlrISFQmrLx/TXD+X/8yBkapYHc2F5Te734Wk2B/A6g5iPcXevhMZGoCt6rc3PLCzKvd+G+2tZlhUCDeUnwVST1mnBdnBwGCSAsAvS1bbP14JhFl9RXss5ODMvWfK2KwwMDL4nMJvTwCClMJvTwCCl6GpzFvvENT59kotirS9L6OO6qlt7/LTYVRfeknqrz337T2ndEAibp6Y4BbAGdkSlTxQlvs/cPQv2VrHMapAcpMpNQh+VxWscFgqgt8tomQtOhdByvK7auGVhaSYj9mih3EfrrlyTUE1Z9Q2plOQeD4EaxFbW0sVXxT7vKXEK4PqGhFJOglKmWmUhc3FA0giLniq6tSVrPVfs1kClpDkYksrxvWqCyL4KBcOGR9gXUIS+MjeXOBVxbVns6eER6FPjsa8BBexhxOfYgfCGZbNthzZnDK0r221WXVHqXYdDKYGPdXHlmtuhCqWAssViUY3luKpe7z4wb04Dg5TCbE4Dg5SiK6198WtfSsaREos2oX5s/wArC6o3LiVjB9oD9qqarR50JO5T6oQY6sfmIPOk02S3eQ5o1uCgEmxfF4p049qVZGw3OcsIc3bKg+z2z4I731PUanlVsomKPUJlK6NM0T904gGYY1pbgxAVmgq6E/JQv4Su6i0+/+lZ+T4ob2Nl8hyOqQwIvdy5tUFz5R5Z296VMIjjcIZNDK3rPKWwCSAbx4Uso3qN6d6hKckCWrjJtZKwXmyxJMeo7rKKow01foJYKUrglRP5ipJC7Z52W84rVDV9KENICfCDlnyuCWZW5CgFjAc1ilQrQjtipc5+MG9OA4OUwmxOA4OUoiutLUNn6/Pz7JGdPiI1c3brTDkmeoXirVwV2jI4wmUn28AC4hy/5ktFoXF+AFTKY49s37Aki2/tcMnLQhFEtyDOPTrNou/WttDJTouzgHygwIHKACnm5AIylnj7lq4s0LpcXpK07Uuc4O96QqqzOaFxg0NM0VuBnL/2BtvwnJYX5fgzIB6wLMvaXpXzyjr8//JOU84/hGwZ21e0NhYPbcfi5+7b8rlVKJvZaPF9G5oQz/+jH/1Bmrt2XTzbtQCykTLs7iyBGdTeZVOnsSfX0or4HEOkoTGUrlRRgA546X2dFA+fc8GsyriqZYSHBhPf7zA2na0NDN61MJvTwCClMJvTwCCl6Gpz3gD+f+zoLM31j4hg9vnnX6S5e++R0MHAAKhGBrjzdIvc1Rw6mL8loY97zoqNGAasTqiDPaPd4UXIvhkEdUmcZcGzMyDhjUgVXir3HtwecLcm373elPHAKNuE09PHknFPH9uSPrjlMxn5v7IdsZ3T3BFbOJtn+6vdlHOeGpbj16vcYTsHxa5sVYvVBoVGDM8lDtk2qkFRr5cvnKO51RV51neckrYQ5y+cp3XLC7LumW9/h+Z+5uf/bjJe2xTFTmebw0eOK8+62dDFsqBdghJKR5C102jIM4tU6ArbGRYKHELDWxJBHd9YpQFhONDRtXu9rlvvzz/ztisMDAy+JzCb08Agpej6bh2AWqk16EJtWZY1MCS09tY617up1cWVXQEB9NQ0C3zXNyVL5fwbb9Dcm+eFMo2MClUrq1YHKOZeWZ2nOcxIKkMWU7GHs5HyUB+1ozKhrs+vJOMzZx+kuX4QJZdKQmVbKgE6AnrTDJgmNoGlVyChPWxzeAC98jtVbn9hQabLXkPo34iq2RRDeMBWmT9XoJvYF770hJy7UitXQOw+e4prGX3sE59Oxl//8uPJeFJ1El+6LibLJz72UZp74mtfScYfePRDyXhb1cht1eXvXJZ/xh0wN1oqzNcGMyLw0UTi+5GDsFahhzOtArgpjSYcT4nsHUeygrwsZ1N5nkl8NzB418JsTgODlMJsTgODlKKrzRmDiLWnyJy5CL1TZo+wLelAPdPxSUkhawUsaMU2ays3uebsJ3/gh5JxLleEjzCvr66JTdhUoYPFy2KbHTsu9XMzqoXe5rqkJlY3tmhutyau+D9b5Jq2iytih49B/dmBWS7iNQRqkEOHOKWuA67+a/MiPP6j//Z5WlfKiE103x130NzhMUmLHOoVOzPrshKiZcl33VxhVcpWR34Kc6tit4bK6Owry99Xtl6juRUQdxcrAzBmG78AdXzPvfESzU2flKJkW+CTqJS4m/cO9LdZXFmhub06pFyq1MGgA2EiSKHLq27hmSy0KcxwWl4GwlBeTq7F5iiflQFBdVbZnE7BqFIMDN61MJvTwCCl6EprCyAQLfdzh+NSSahKj6IEDciWwZL9DdXWbmdXVCQnjx+juZPHpJXChevi5i/3sAvahe7E2Yh5xQBQh/EhoZYdddmHpiT7KZtjt/lDx4RC3lzmmjxfefK3kvHXn341Gb904QqtGxqQMMvkoQmau/OMZNL8vV/628lYU6T+opzXzNgUT8I9aDUg4yjP96ody//Fi7f4Wr7+rNRVuueDH0/Gx06cpnW/97nPJuM3nn2V5gKoyXP/aaH5F6/y/ShkhRrXWxwyOgWZRZevyXPv7PD5NpsSSolVDaEG0Nq2yvjC2j09QJVdRTstMAlClfljg6InB9lmGfWuc12Zcz2msWGGTav9YN6cBgYphdmcBgYpRXdvLWQxOEXONqlWhaIeHee6O5ur4jXNDwkFW1J1a06dEArTUO0e5q6+mYwjqOGSzTGF3tsRL2+tzrT5zrseScYdX2hLx+bv6p8ST+5gnpPWs0DZy/1MRf75v/xXch635Jr/7GnuqrW5LgncrsuZKD1APT/767+RjGcOc+wUkQsAAB8NSURBVK0hFE4P9vE53rwptLGUwSwj9o7btnx3jzIPPvCA0PdXzksNqLLL5sZnfuiHk/GuKms5WBIquLMtnnK7wx7fraZQ1Olp/l0tr8q9KkP9pps3WcA+VBGavxCw4CHClgsxJ/i7UI/KAVMhVh78jA2/fYe93hbcRxvECkhjLcuyHKCugc3vwSDQjSJuh3lzGhikFGZzGhikFGZzGhikFF1tzlWwHfcidicPDYvdE9Y3aS4GYWlfRT4XxCO0rgodmZdvXKc5LEDVXxa7r11j13itLhk95T6VRVKXUE0+Lxy/1ebz9TfFLtle4vNYOSf26eSp99NcoU8UMn1Fsa2/v/gRWlcuYXiGYyQ+KEraDbERS73s2o87co669V4R2uFhG4dQKWxcUMQcG2bh++lZuXdnj4qdWWuxwsZfFXvxX/zDX6K5W+sior5w/uVkfO0qq5YevF/ULK2Y7cUSPOvmnoREmirTB7uAL60oUXlRwnxunkMYcQa6dkOmj24t4VFWEL/DIrBjbVd+69rmDECI7Yf83LGY20Ewb04Dg5TCbE4Dg5SiK62tgWj10U88QHMLNyVZvNHh0MQVaIPgQofm0SHOjvnmn34tGR+d5RYG0CTZ2toU2pKxuJ5LDMncTRU66ANBcV8vtAfYZpp19bW3knG0x3O7VaFW5X4OGTkQdgmhqUO5rM4R6gGFqgaq4woNLfTIRcex6uQMdDVWtZKwXYUFbQpi9V1ZEJlrOhnl5F5VgBZWYr6W4VGhnU7Aps6hYQk5BG0xNzyHKeOlK1eT8fRRrmV8aFLMg7nzEk6bmOKsqHXoivbwex+huSsgIPBjvgdVEF8XChBWURk7LtBfRwkIUGyNpZgCVW8pBlMw5/ExslmTIWRg8K6F2ZwGBimF2ZwGBilFV5sz8MUuiZToNgu9TVpNtl+mJo8mY6z7em3uAq277567k3G1yn1OwrzYKWffIzbFXpVF2cGa2MVjMyz6tsHWWV8TuzVosI2MXZiHcmxHuVCsa+XGPM2VxiTlLQsqHTtW9UvBje7Yek5sIpvqnPL/myDusXyL77cNPe86dZlzHU5dQ/WGk2P7KIRjZjMQkop0+puEeGyXz/HUabkfL78qtYz7Kmy3NqGNY6WHQx07e2Kr1qAe7/gYh342dsS/sL3Fz7NcljDLjlJCeSi2tuWmurrgFszFql5xAJIh7D6obVMvK8f0lE2rbeH9YN6cBgYphdmcBgYpRVda24ZXu6Uy//2ahBguXLhKc499Qur/uEDjJg9x/dINUHKMTxyluVP33p+M9zaEys5feJ3WVTeEthT7VEs6PGdfaFB7l2vwYon+IGb60TcoLvzJmRM0l4XuzTFQS09RUuy0HN/WhVkmuT4S0848qFeCOtc56oCguAGtCTyV9WJDl+esrVs6yL1CFYYT8U/Ege7VzZAzkNZvSebVVaiDOz3JYbLrNaG1ywsc/tqFdn57kA3W2+RMpX7oYt5xmBrPXZXv7hviIgEtCG84eaHvWO/HsizLBVWK5fK9yrnY7kH+3Y6Z1rpwH6OYfxO6dcN+MG9OA4OUwmxOA4OUoiutvbwkgtm+ElOk129KB7LZ46doLgavVR68n0trTMeOzop3NVNhb9z6utCb+dfF87e9zq0I8kXJMJk4xNT4W99+MhkfnhAPXt8IJ+APRkJvttfZuzcyJPWFCh6XeIyg9QEJclUCtA0C69hVIlsQ7sZAE2/rWAVzddUao7Mj59zpCHUtVYZpXQFaRkSKkjqQJN8EaplRmS0ZpHiqTKkF3uAc3IPVlWVaNj4mmVarq1xudOnKfDI+Pj2TjHd0JzG433GGf8ZuQTzKm8qTG4Kpli/I8wyVh92FY+om1C54qV0ySzR1RSjvuP3270Xz5jQwSCnM5jQwSCnM5jQwSCm62pzTh8Rmqa6zbbC6LsW6pk7fR3N9A+LmXr45n4yPn+BQRAb62m3X2B5tNqWgU6cpGTzXlrj0/o/+lLSdqzbYFusZFNvSzot9EakiW04g52G7bClE4GKv1bh2qgfZM44vNlFc4KJVFtptHts2DticKOLtNDnE4PhiO62v8T2YGBZ7uhRDlpEKC7V25Rx1gS/s0IxhFjvk+xF0JKTRUfVi5xckhLG3J/eq3WahtAutJTarrAJ69XnpdN3f8+Fk7Pus0sn3i69hdJTVTvPQTmJAtYJY25LfSOyKbXpb+z7ICrJV+AszsrADdhxrVQr+pXs1KEN2H5g3p4FBSmE2p4FBStGV1t5xQsIIN+av0Rxmn4yoFgMVaN3QgVqyGVu9yqH1QdlhCnbjvHSf2rkllPf+B95L6778xS8k4w9+iGv8fOzDjybjGFz7Lz3xDVpng0B5YpzDMREkrUcRJyv70H3ag0TmIOLrdAsQhrI5myVyhELGQGszOjzgQSilrTpngzB4b1coXanAFHr+qojgd6FlgWVZVqko4aQzd0CNH1W7B0MrKBiwLMuqViVDKJ+X+x1bTH8zUOv1scc+RHN+S+j8Fx+XTmsf+zh3wM5ZQlere5y9NtAn99tXGU4eKAjaLflcoYdrATtgHqj8fsvWvTISKKEB1hpSv/1Yh6H2gXlzGhikFGZzGhikFGZzGhikFN17pXSE/3uqzmarI3ZE22f+vFkVd3UZ2qxtVble7Oiw2LTXLszT3JNf+koyPnn8rmR8/BirDD50v9geC29yS7rrLz6djJ228P+jKs2vCalbQZNtLFQkxFmVggWShBjCBQWbj4GZYZGtXPZgu3tgd2sbpQGhlXsfZtt6uyphrf4xCWM19ji0NHVUfAM7e3yOExMyVwfFkTKfrSYoYDa22OZcA0H70WOSmnn16mVad+uWpGC+9RYL8A/PQk+YIXm2Lz/3TV53QoTdo6e5+Fw/lPxtKzVINi+/n+09eBaqV4oN4bVIhVJie3+htK3CJXGM6ZhqrmNsTgODdy3M5jQwSCm6ZwgdFvXA80//D5prNISr1atMkQYHxF2911xMxq5q3/fMy0J3fvezn6O5n/jox5Lx1auSEfOr//TXaN2pQ3IJf+njd9PcUL+0S4gyQme8Iitsci6ss9kdnsmBOsFSVARq9MTgXg98zu6xkN6omrMZbAlQku/SnvYMCIN3mqzQcKGek4v1bRV1KlTkGI0WX+fOjlDZItRD2tnjztPbdQnjPP7fH6e59z8smWL90BpjdYG/qwjKnPUFrgk1NS2C/MFRUSo16qwuuQVtFXdr36W5w0eknUSYU2qnQMyzgQnJMtraYIpeg7q+ba02Aa6P5ocOuYRAZXVrDMvSf98O8+Y0MEgpzOY0MEgpuntr8dWrPFYBUKZDqrP1wIBQyCsXpaR+bw93zvq3//rfJONGnangtUWhLa+cm0vGkc3i33JFvJPtQGXmAB0ZOiR1bCKXk6HzIGwOQ/bE2dDSQZXdsQJsswClQ33V6sABoW2sRLeRJRQVuzBn83yvMq5Q11KWs4waIPoOwXPuhHzC9U3J1rLVdfpwLTWovdRq8XO5fkW6Xv/EZz5Ncw2obXT+Tan1VFRJ9l5GzqukSmOG4PWOgPIPDbNw3Id6SJ4ShPtNocAdn7up9/bPyHdDi468w+U7W5HMXbjOYvFcVih7AC0vdAsNunfKS5/LqW7Z+8C8OQ0MUgqzOQ0MUgqzOQ0MUoquNmcIdkijxnzay4ii5IKqJTt9n2Rv1F8C26bF7upJcLe/cpMFxIU+yVhxMqKIGRllrv7GRamZ60Yshv7wD0r93KFJca/rDsQdqLdqB/z/FRZ60pkiFtiBMSgSMjGHDnywR0OfFSU2fA5r2gaqo7QLNlDHZ5t2ZUXuXQtVKSqrqw0CaDvD51jok+dZGpBx3uN1zzz1RDJ+9BEW2aMdNQY2Yr3GYZBhmJuY5HNEl0Ihjz9PPo8SZJ45yubshY7jXoUzyuoQ+gjg99hfKdO662tyH3uUr6QFNXRDbLWh2l9kMQyns4zUb2Q/mDengUFKYTangUFK0ZXW1uuSOJ3PsRh1fkFc1JUZdlc3oM1CFEJ9UYszWx46czgZv/DieZrL9UMCN7irOztMjcuQ7bO7zW7/Z59+NhmPHBZaWypwOCPoQAjDYwoTggtcExFsvBYBXXK6UBbdPSyCkEYdQgDnXjtH6zqQw14scyjo7nuEXu5GUJdV0ffr80J/5y7x/Z66U8QAJ/vulPNTAvNPfeoHk/HlC8/Q3NCQ1DKKINSWVbQzgGseHeAawu2OUO++MjwLh3+q2byEPmI1Z0GoZrPKv83BKfk+H567rTuOwzF7y5xR1m7dgnXy71HMWT8R1Fi6ndZabwvz5jQwSCnM5jQwSCnM5jQwSCm62pyDQ1J/dW1FdZ6G1LC3LrJg9r2bwskfgIJcz3/jP9O6HldsxAfuP0ZzObCrUC/bqHO4JJcT8h6qjKjmhtRE/fLv/1YyvhNsXcuyrMEpCf2Mjc3SXAz/f0W6aQbUNsVWcLFStqCdGageJTEYrjmQNbzn/ntp3e6i3Kuccu1v3xK7KgfFvmylpjh9Qgp3zUAfEsuyrEZOLqAFF7O1zc89gJS6noKyJTviUygVJYSxtc2+hsce+0gyvnCRbes8hB+uXxHV0um776F1BQilxMq2brUl1ORlOFSztyUphpmM2JKOSqvc3YZwSZ6vE9VJGG50HF2LFloF6jCc/i3tA/PmNDBIKczmNDBIKbqrUuoyvb3LYYoYqE9RCXeLIDxevSVtHLYb7K6u9Is7PBexYPuVp0VA+5FP/Egy/vxnf5PPA+qQ1lU9URfaxhV6hEasLjJV87LSRmCwyBTJ8aBtXoaVC4EtFNUG930QcQYPUphYUR8Hzj+E1gd+qNoPQKJL1lXt+zx4jCAEjlQmURPodSfPjx6PkS0IFWzUFS3cEArtqLYTtR0IoYEtst5iSvepBx5JxiubbKZYjoR7mpsi9G6qkEh5QL7bU+30crE8pwWXz3EQKGoNWil21HMpQUfsvbaaq0jIqL4r2WWhUvDYYAJgCwfLsixLddLeD+bNaWCQUpjNaWCQUnSltX/yld9Nxq7NXcCmp8Xjub7HJS/Pvy4lKQ/NiCd0ZnqI1q3NS32hVoO7TR0el7XnXn4hGReYdVqVXqEpQYdpRQsK8bQCWXf+4k1aNzQq13LxLW47MTwuIu2eMtMzF1oORCF2r2bK4kDLBVt1rA7C/YXYGSW2dnNCpWyVXoJdqWNfxlGHzYhcj3gnbYvnQkdo9JUb4iWt7XJ5zeEheS7FQa4JtXRFzAgUi3t19tYuXgcv7NFJmnvxBclc6gfarLtG1xtyD0bHuU5QWIcu3VX+7u2O3P+cJ/S34zN13diU666xBoFKYDqgwPdV4Sf00MbqPeho5f4+MG9OA4OUwmxOA4OUwmxOA4OUoqvN2WoKr+8rs2jVc+WjE0OsLLhy7koyHh0+nYy3q7doXashIY2RfiW6BXXC2rwIqrHmqWVxWGFXKQtKvXLOLVDx/sCnf4zW9YBq4ktf/iLNjY1LcafxKW51eGhySo4BLfTiPN+rDIRZvJy65RA+scHm1NlI+RKEDrJsr0SBGEUxKCFada4524bWgXuq9m3bl7W7O/KcFhbYPi/1iZ3Wk2O7uBPIefWW5XyP9o3Suv6KfG7+wkWaG8RntiPnm/X4PVKGztaBq1Qjgah78hme6+8T232vIeE7N89hshw8l2bAvoyIumyDGkmpY7BWretyllFkxNYGBu9emM1pYJBSdG/HMHs2GZc8pp3bGwvJeKfG2RsRZL3M3CmhlE5tntblB4QeVIo8198jx+jsCf0tq+T2EBLJcwV27f/ML/5yMj5116lk/NJT3LHq9z77H5OxqzzcW9sS4pm79BbNDQ9L64AjR08m48NHuYtZuSxZRnbE55jxoG4rCbYVde0IVYvVY3NA5IvUVTvrkSgXVUzqDz//X5PxAw9KWwvP4UylNahXdPQwiwTKefmNYD3atxZWad1Lr72WjMcrfD/CbTkvpJOuooHbuxLamxnhUMrNa/LbHBk/wceH0NV2Te7b6i6H8moxdOZWtZIdeqfJ2FbvOpvCJW8fOtEwb04Dg5TCbE4Dg5TCbE4Dg5Siq8151z1ic1658AbNbe5A/wibXc17e2If/cFnfz0Zf/iRD9G6hTWxX06fOkVz+ZK4wDtNcfMPjLJ98Tf/xk/KZ4qqvigIs5/+6peS8fAg1yj9Mej5cVEJxy/PiZ0ZqF4s66sScli6AUWfvvttWnf33SKcnjlyhOZGx8RurVTENq1D/xPLsiy3JW5/P8s2UBbUJiEIjW/rjg1t9HaarAap9Mq981uyrlzkEEMcYm1dfu4TEGo6f15E1P0T07SuF2xwX/XNK0FY7vC0CPCru3y+N29Iz5aJwyye74MiAaFSg+xCDd3NLfldOTn+TbiR2JzB2zeh/l8HUe+6qMucCaUYGLx7YTangUFKYXcrC//a534mmVy4ypkcy/PSlm+wn+uoNtuSqbPXlnf7HWffR+sOVWTu3FsLNLcdSCbHyLiMN9d43eKC/K2zh07OziTjUaipWu5lCpMpCHXbUfQJVSS+z2GFixflHly+LFlRDSWUbjSE/kXq/8NsTuh7sUfuY2WIFTxlCBcM9nH4oZCRY+YLEsLY3N6hdS+9+moy/tjHH6O5F54VcTu2cRgbn6J1OzXJLOpYfJ3j06IwKUIbh7IKx9RBGdI/ez/N7S2/nIyx/WK7zsqnCEJos/fytex5Qq9XljhLanVTwn5IyivDTI03tiHbTClI4hDaFEK7x1aTTRHu+qdVKWJR/uN/8Cv7FhQyb04Dg5TCbE4Dg5TCbE4Dg5SiayhlZUnCJS1Vs2piRvppbG1x+l4rkhSy9U1Ji3r6qWdp3ac+/lAyDmJOJ1tZkmO++IJ8ruRwO7mTsxKK6O9lt38EIZhGXY7vetzq3LPkfD3V0j0GdUhepbx94JFHk/H73i9Fq96a41qsr74i6Wq7SlbfbovttNcSe/fa1SVa5xblmK4qZDYyKkqO8cNi980vc1vFiVEJabz2IrdtPAlhi0JOrrNYUq3xrrwk33uYVTqHJ8Vuq0PKZeyz3VfpE9s6DLiwmwf2egRhm+YWV2TwfbEYO7tsW9cdeYZ7Nf7hvudhqaM8d0Pu8dVFVkxloe6ubpKDv4mY1EP6XQcfjHnuHZStNW9OA4O0wmxOA4OUoiutPXxY3Oi7eyxk/sPH/yQZ6/DDHWeOJ+P+EaFST32Hs2/yZSnm1DfCCoevfufryXhqRLJXztzBrv3xIaFIfkvX1oV2CeDW1oWYMrqFHB4DBdAq4wbJZQZUO2fPnqV1J09I9tPyGocEMASzvCzUqtDDFLoAmTTjIyzmPgX324d6wk31XXMXJNtpaGqG5i5861vJeKAi9xTPybIs6467RWU0O8Phh/4+zMyR30tT1aZ1YK7Sw++HJgjJ1zeFGuc8vvd7VaHKC1ev0NzwXfIbue89/CwWluR6tqpiImWzSh0DPxhXhVJIbQL81Fb1c1E8b6nfTmy//XvRvDkNDFIKszkNDFKKrrQ2gJo8r77OQuPZ45LZ8fwLL9LcS69JXdLjx8V7WMpyG4FTR8TT+q1n+Bj3nBJqMlSQz40OVGidV5Qk6jDi2roBeP46HfHalVXp/SxkxFiKbrQh2ymTZcF5xhPqmcnA2GOK5GTlGDNl7hA+e1KE2S5Q11i5CMMm3Dvl4dzdEo94qylz3/+RD9K6f//bjyfjpXn2Bp85Lt7aGrQY6FUe6t1NmQva7And3pT7j8L3jRafb6UfWiTE7HkuQ42iLaiH7Lj8U+3tg7q4BZ7zffm+8xfepLk21C9uQfaar9ynsSP333WVaxXXohc27vau6ybEfiefMDAwSA3M5jQwSCnM5jQwSCm62pxhS7J0ylm2gb7yxDeSserwZkWu2CIf/ogoUR5773Fat7YhLvZjxzmUUl0XO2q4CO3pcsz/c3no/+GzbdNponoAimDFbCv1gHzAybAtkHXE5tJ1ST3olYJ2SKC6JFvQeVlpi+kYQaDSsACOK/e/1WE7rQVuf68kNls+x2qhv/OTP5uM33jtFZrLFMS+O3L2wWRc2+PnfumqZDtVN7goVqFHvvvwkPgMqj1cO7bRljBIJuBjVLJyzp4n/oX+kTFaV2uIIqgnr9rpBaIOsS22/+t1mcsVJfupvsvXmc3AvQt4zobfkhXhnAq1wW/Csdl2z6pu3PvBvDkNDFIKszkNDFKKrrS2viO0cHGdM0UGZiXpeXSC27g9dL+0YMgCHavWOam8DdksjnJX95WE0uRBrOs5nNzuQG2WXI7pTRtaDnQ6knAeRhzSCaFVWzbD9XlQi67btuEc1uu5LVMEQzeqlkwM7vcYKHSkWks4QJmwA7ZlWVauIGEFFIfne5jSTR6SMM76AmftTMxIaCy0Za6Z4ZqzoyMSBllbmqe5hRuSqRP5D8hnhjl85GTlmut7nNB+c1ee0wgk0nc6LHjweuSYnUAJ2IFCeh4/z0Jenn2tLcesqJrHnY789m2Xf7ch2Caov9dCehu+29NiC9UmYj+YN6eBQUphNqeBQUphNqeBQUrR1ebc2RK7Z/LUXTT3iz/3C8l4fn6e5p56/A+S8XRZ3NVbdY651BpiE95avU5zYU3c7UNTEmYp9XB4wIIW726Bj2+D27wD7diDNodc0HZ0lc2J/39l1Bzad2j7Rsqljvaom1Ft4sDedWBdGKn0PThJVJ5YlmVl82LPYBGyWCkhcmWxxZ55cY7m7mm8Pxk/8LCEvIbH2DZq+qIyitR5tMFO24LiYsvXq7TuKPSScW32E1QzcowqpO9VyqooWw7T93iu0ZZnsdXke9Bblrq4G0siRu8p83X6HblXnYh/VyGE3kIIr9mqvaON/gVPPXdH/85uh3lzGhikFGZzGhikFF1p7ewJoR/2AHcn/me/+ivJuFRkqpmFsv+LUN8l8JgeeEALOw2mmkFThNPFfsiwUQoEJyOULgyYZlngyg5AGdGuc1fnAOrtOypzAzsS31bjF0IrRCHVOvxcqGraBkgNHaS1vC6GvzU1xghPFtQbrQ7XK8oXJQPpMz/1QzT3jS9ITVsrkuf36Me4hZ7XK2GWyObjt9piRhQhW8hX57G7KeGT+p6qEwyiar8t9227yUL6HEjdsxFnVkUQKgvqXEt2a0+UMyenJIvp6jKHdAJX6hzHKjPMgZaAWUvGOoSG/Rhsm38T2iTYD+bNaWCQUpjNaWCQUnSltV/+xh8n47rytPYVJRE5bDCtsB3hWXuQjB45XCJxfk4o7/TYaZoLS7K2p18otaOygGz01rqc+eOBFzMC72/Y4ewbprWcBeSCly1WYtoIKGoMFD1QYmj05DoqQwj/jh3MMlLiXxh7qssYJsUjaw46fD+qeyJkGD3ST3PnLknNpqL7Y8n45e8yLbzvMalfZHs8l7XAJABhvZfh5x7l5feScZSp0JS5GiT4N1UmkeWLabK7xdlDbU+8vJMFvs5ij/xe2quybsrlLKZtaJewG3P3ujZQWaw/FSvBdgh0O5flrVYpc9GA/WDenAYGKYXZnAYGKYXZnAYGKUVXm7NYkBBJ0GE7yq+LHbFRZXugD4pwDUGN1e8+w0Wl+mIJ1UwMsNg6VxIRroutGmI+ZdcVuyS0OAzS2yd2cbMh7nxf2YRuCDazo5QnUKzLV+aR54HNCTVzg4BtPVTLuMqWDDD7BEIuXoavpQYtGArK7s5Y8n0NaJWXV+Lz1h4KttlO+8s/LhlgVy9I64fqOp/Hm0/I/+f9J7kuru/cTMZxIP6EnU3+rjgUm81V7QEL0J7RhWeRdw/2NfSqZ5bPyd/ZAtvnXg7Ca+BrCC22n5tNubZtXcjMlnOsevIb65S463oIYa3TJ1i5VS6yCmY/mDengUFKYTangUFK0ZXW+r5QgBs3FmnuxPGTyXi9ypRgYxO6e0FyyPjwEVoXbws92Nlmanx6WubsjNA2bHtgWZblQajD8/j/GqSQhbzQiMYOZ5sEmFmkwiUu0CdbFwDqyHEwo8e57b88oU+xqtMaQ4dmFAbHMYeuihAm2q5yV60ahIl8qM+b1UJjzNpRWS9nH/5AMn7ofTPJ+No5vlc3Lor4uhIO0tzEpCSVB5aEbQ5NqRAU1OSxVZaR35IQSbstc46uz4Omg2qvYYFwP7aUOQYi9hDOI1IhtGxejjGQ53vVb8sxJ9rzybi2zeKNhi2hvDMeh2o2mpylth/Mm9PAIKUwm9PAIKUwm9PAIKXoanP29Ukvkzvv4nSjW+viam53mPOPjovbeBvav5U9tlFmT0g7ub6yTgGUz9lgb+mOwGgTZpRaIw9pbpWKcP6WSgVrNsXVH6jCWkVXRLiROn4blC5o9wWqgJjTkpPeqrLw2G9BV22wEfMqRa+QxVAKu+GLY+LO9yD8Zd9WVErsKls9eRuuM2PJs37g0TO07vwrn0/GVy7ydd7z4IeT8Y6/kIybLU7fC8Cu7LTY5gx8+R10fHkWsVLzUD9ppQbJgI0fhGxzRlAcDcVDmH5pWdz7xlL2edaW6x7MyLVkM3weVbBpL3znSZrbBv/I3Q99v7UfzJvTwCClMJvTwCClsG8TEBsYGKQC5s1pYJBSmM1pYJBSmM1pYJBSmM1pYJBSmM1pYJBSmM1pYJBS/E9FhwbF7Unf+gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architectures"
      ],
      "metadata": {
        "id": "pOeojm615FW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DCGAN"
      ],
      "metadata": {
        "id": "o1fAbtpW5PID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.discriminator = self.__get_discriminator()\n",
        "        self.generator = self.__get_generator()\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    def get_generator(self):\n",
        "        return self.generator\n",
        "\n",
        "    def __get_discriminator(self):\n",
        "        return keras.Sequential(\n",
        "                [\n",
        "                    keras.Input(shape=(64, 64, 3)),\n",
        "                    layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Flatten(),\n",
        "                    layers.Dropout(0.2),\n",
        "                    layers.Dense(1, activation=\"sigmoid\"),\n",
        "                ],\n",
        "                name=\"discriminator\",\n",
        "            )\n",
        "    def __get_generator(self):\n",
        "        return keras.Sequential(\n",
        "            [\n",
        "                keras.Input(shape=(self.latent_dim,)),\n",
        "                layers.Dense(8 * 8 * 128),\n",
        "                layers.Reshape((8, 8, 128)),\n",
        "                layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "            ],\n",
        "            name=\"generator\",\n",
        "        )\n",
        "    \n",
        "    def generate_samples(self, num_img):\n",
        "        \"\"\"\n",
        "        Generates num_img images.\n",
        "            Returns:\n",
        "                imgs: an array of PIL images\n",
        "        \"\"\"\n",
        "        random_latent_vectors = tf.random.normal(shape=(num_img, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imgs = [ keras.preprocessing.image.array_to_img(generated_image) for generated_image in generated_images]\n",
        "        return imgs\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "u2mfR4Hk5cWi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE\n"
      ],
      "metadata": {
        "id": "JYsq8qxt7y0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "u2Hp2RCkK6cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, latent_dim, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = self.__get_encoder()\n",
        "        self.decoder = self.__get_decoder()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "    \n",
        "    def get_generator(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def __get_encoder(self):\n",
        "        encoder_inputs = keras.Input(shape=(64, 64, 3)) # avant 28, 28, 1\n",
        "        x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(self.latent_dim, activation=\"relu\")(x)\n",
        "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
        "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def __get_decoder(self):\n",
        "        latent_inputs = keras.Input(shape=(self.latent_dim,))\n",
        "        x = layers.Dense(16 * 16 * 64, activation=\"relu\")(latent_inputs) # avant: 7 * 7 * 64\n",
        "        x = layers.Reshape((16, 16, 64))(x) # avant: 7,7,64\n",
        "        x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(rate=0.25)(x)\n",
        "        x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(rate=0.25)(x)\n",
        "        decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "        return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "    \n",
        "    def predict(self, input):\n",
        "        z_mean, z_log, z = self.encoder.predict(input)\n",
        "        return self.decoder.predict(z)"
      ],
      "metadata": {
        "id": "BeisHTws72i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeOQ2YkyBDC"
      },
      "source": [
        "## Training code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "d1t400Il7-e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ejwQuN9HyBDB"
      },
      "outputs": [],
      "source": [
        "class SaveImagesCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128, epoch_offset=0):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.epoch_offset = epoch_offset\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch_adjusted = self.epoch_offset+epoch\n",
        "        if (epoch_adjusted % 10) == 0:\n",
        "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "            generator = self.model.get_generator()\n",
        "            generated_images = generator(random_latent_vectors)\n",
        "            generated_images *= 255\n",
        "            generated_images.numpy()\n",
        "            \n",
        "            dir_name = f'DCGAN_CelebA-e:{epoch_adjusted}'\n",
        "            img_path = DATA_PATH/dir_name\n",
        "            img_path.mkdir(parents=True, exist_ok=True)\n",
        "            crnt_timestamp = str(time.time()).split('.')[0]\n",
        "            for i in range(self.num_img):\n",
        "                img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "                img.save(img_path/f'generated_img_e:{epoch_adjusted}_{i}_t:{crnt_timestamp}.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "zFJdp0zi8Ah9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Model configuration\n",
        "load_weights = False\n",
        "weights_path = MODEL_PATH/'DCGAN_CelebA-70+30'\n",
        "latent_dim = 128\n",
        "\n",
        "## Training specifics\n",
        "epochs = 30 # In practice, use ~100 epochs"
      ],
      "metadata": {
        "id": "tuJ0XRezBTqA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_GAN(load_weights= False, weights_path=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        load_weights: When set to true, loads weights\n",
        "        weights_path: The path to the weights\n",
        "    Returns:\n",
        "        gan\n",
        "    \"\"\"\n",
        "    gan = GAN(latent_dim)\n",
        "    gan.compile(\n",
        "        d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss_fn=keras.losses.BinaryCrossentropy(),\n",
        "    )\n",
        "    if load_weights:\n",
        "        gan.load_weights(weights_path)\n",
        "    return gan"
      ],
      "metadata": {
        "id": "sM46ZeXR-FTp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan = get_GAN(load_weights, weights_path)"
      ],
      "metadata": {
        "id": "xl0sehedAMtT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_images_callback = SaveImagesCallback(num_img=10, latent_dim=latent_dim)\n",
        "mcp_save = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH/'DCGAN_CelebA-{epoch:02d}',period=10, save_weights_only=True)\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[save_images_callback, mcp_save]\n",
        ")"
      ],
      "metadata": {
        "id": "z4Op5ie79OU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z67zpcrgyBDC"
      },
      "source": [
        "Some of the last generated images around epoch 30\n",
        "(results keep improving after that):\n",
        "\n",
        "![results](https://i.imgur.com/h5MtQZ7l.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1hiO3kYLJvoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "32_LvJVcZqcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights=True\n",
        "weights_path=MODEL_PATH/'DCGAN_CelebA-01'"
      ],
      "metadata": {
        "id": "pWGmyfGSCIdp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan = get_GAN(load_weights, weights_path)"
      ],
      "metadata": {
        "id": "pkDBjg_PZ--v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating images"
      ],
      "metadata": {
        "id": "65FPnXLZfSYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_samples = 100\n",
        "\n",
        "# # Generate samples\n",
        "# imgs = gan.generate_samples(n_samples)\n",
        "\n",
        "# # Initialize directory for pictures\n",
        "# dir_name = f'DCGAN_CelebA-generated'\n",
        "# img_path = DATA_PATH/dir_name\n",
        "# img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# # Save images\n",
        "# for i, img in enumerate(imgs):\n",
        "#     img.save(img_path/f'generated_img_{i}.png')"
      ],
      "metadata": {
        "id": "lRcxXYZtbzxp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving random images **TODO fix with test**"
      ],
      "metadata": {
        "id": "biMA_U42f7ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "train, _, test = get_dataset_partitions_tf(dataset, size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjBc-WQ1f7y3",
        "outputId": "9b762abf-5f9b-42d0-c797-9ffcb0fedc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_name = f'DCGAN_CelebA-real'\n",
        "\n",
        "real_img_path = DATA_PATH/'DCGAN_CelebA-real'\n",
        "real_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "gen_img_path = DATA_PATH/'DCGAN_CelebA-gen'\n",
        "gen_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "i = 0\n",
        "for image_batch in test.as_numpy_iterator():\n",
        "    for j in range(image_batch.shape[0]): # batch of 32\n",
        "        # Generating img + saving it\n",
        "        img_gen = gan.generate_samples(1)[0]\n",
        "        img_gen.save(gen_img_path/f'{i}.png')\n",
        "\n",
        "        # Saving real image\n",
        "        img_real = image_batch[j,...]\n",
        "        plt.imsave(real_img_path/f'{i}.png', img_real / 255)\n",
        "        i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "7HfX2OlfhHGu",
        "outputId": "3d4dd567-c44d-42c9-a104-72d2f31cc3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-d319110a1beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Generating img + saving it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimg_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_img_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{i}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9d5dd8728877>\u001b[0m in \u001b[0;36mgenerate_samples\u001b[0;34m(self, num_img)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FID"
      ],
      "metadata": {
        "id": "jMTi8OaIZ2Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yBK1xSWZscB",
        "outputId": "3892be38-fbb0-4e0c-d09b-8b8e0914299b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch-fid-0.2.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.21.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->pytorch-fid) (4.1.1)\n",
            "Building wheels for collected packages: pytorch-fid\n",
            "  Building wheel for pytorch-fid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fid: filename=pytorch_fid-0.2.1-py3-none-any.whl size=14835 sha256=5090abf0d28a2ecc5e60f0bebd08b90fda9e408801d191841ef0be954cba2519\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/ac/03/c5634775c8a64f702343ef5923278f8d3bb8c651debc4a6890\n",
            "Successfully built pytorch-fid\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid $real_img_path $gen_img_path --device cuda:0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p5-aPZEZwOw",
        "outputId": "da2dcfcc-aa3e-4a47-d3d8-daac0a9eb839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:05<00:00, 17.2MB/s]\n",
            "100% 719/719 [02:53<00:00,  4.14it/s]\n",
            "100% 333/333 [01:12<00:00,  4.60it/s]\n",
            "FID:  51.430461225018604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FID:  51.430461225018604"
      ],
      "metadata": {
        "id": "4_SJy8c9ADtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GEI8C9h-ZwRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specificity"
      ],
      "metadata": {
        "id": "CAlwv1T1Z3-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_img(reference_img, dataset, dist_func=None):\n",
        "    \"\"\"\n",
        "    code below finds closest image from reference_img\n",
        "    takes 2 mins for a single img ...\n",
        "    \"\"\"\n",
        "    closest_dist = 1e9\n",
        "    closest_img = None\n",
        "    for image_batch in dataset.as_numpy_iterator():\n",
        "        \n",
        "        truth = image_batch\n",
        "        distances = [tf.norm(truth[i]-reference_img) for i in range(32)]\n",
        "        i = tf.math.argmin(distances).numpy()\n",
        "        min_dist = distances[i]\n",
        "\n",
        "        if closest_dist > min_dist:\n",
        "            closest_dist = min_dist\n",
        "            closest_img = truth[i]\n",
        "    return closest_img"
      ],
      "metadata": {
        "id": "oKJHxoE6Z5sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "min_dist = []\n",
        "n_samples = 10\n",
        "imgs = gan.generate_samples(n_samples)\n",
        "for img in imgs:\n",
        "    closest_img = find_closest_img(img, train)\n",
        "    min_dist.append(tf.norm(img-closest_img))\n",
        "print(np.mean(np.array(min_dist)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiqXMPgvhdWS",
        "outputId": "a9bb479e-944b-4580-d265-2798949df9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5223.487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5223.487"
      ],
      "metadata": {
        "id": "RWjXGDsJVzVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "1uCD5-iHhdpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb\n"
      ],
      "metadata": {
        "id": "7JJbZw1ghd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3e9TJNOhgar",
        "outputId": "d8764aae-1e0d-4f06-8601-fcf2dff5b89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8WHoN2O9hi-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oCLBll_NyBC1"
      ],
      "name": "main",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}