{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaRrf9VjyBCw"
      },
      "source": [
        "# Main notebook\n",
        "\n",
        "- DCGAN Adapted from [this tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/)\n",
        "- VAE adapted from this [tutorial](https://keras.io/examples/generative/vae/#train-the-vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCLBll_NyBC1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LCwF4rBVyBC2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def is_in_collab_env():\n",
        "    return 'google.colab' in sys.modules\n",
        "\n",
        "if is_in_collab_env():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT = Path(\"/content/drive/MyDrive/INF8225/notebooks\")\n",
        "else:\n",
        "    ROOT = Path('..').resolve()\n",
        "\n",
        "ROOT_DATA_PATH = ROOT/\"data\"\n",
        "ROOT_MODEL_PATH = ROOT/\"models\"\n",
        "ROOT_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
        "ROOT_MODEL_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "hNPxBlC227lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852beefa-1633-4c62-f79e-63a2d68c1fe2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "class Dataset_names(str, Enum):\n",
        "    CelebA = \"CelebA\"\n",
        "    Anime_faces = 'Anime_faces'\n",
        "\n",
        "class Model_names(str, Enum):\n",
        "    GAN = \"GAN\"\n",
        "    VAE = 'VAE'\n",
        "\n",
        "def get_model_dir(dataset_name: Dataset_names, model_name: Model_names):\n",
        "    path = ROOT_MODEL_PATH/f'{model_name}-{dataset_name}'\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def get_data_dir(dataset_name: Dataset_names, model_name: Model_names):\n",
        "    path = ROOT_DATA_PATH/f'{model_name}-{dataset_name}'\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "iSsKnXFMXWXp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selects the dataset and the model that will be used."
      ],
      "metadata": {
        "id": "HAJdc_fgVaF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select dataset + model\n",
        "dataset_name=Dataset_names.CelebA\n",
        "model_name=Model_names.VAE\n",
        "\n",
        "# Get/create their directory\n",
        "MODEL_PATH = get_model_dir(dataset_name, model_name)\n",
        "DATA_PATH = get_data_dir(dataset_name, model_name)"
      ],
      "metadata": {
        "id": "cUnqb5rI2-rE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPUbJUhByBC4"
      },
      "source": [
        "## Prepare CelebA data\n",
        "\n",
        "We'll use face images from the CelebA dataset, resized to 64x64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T9VZevZayBC4"
      },
      "outputs": [],
      "source": [
        "def get_celebA_dataset():\n",
        "    ## Download dataset\n",
        "    os.makedirs(\"celeba_gan\", exist_ok=True)\n",
        "\n",
        "    zip_path = ROOT_DATA_PATH/'celebA'/'img_align_celeba.zip'\n",
        "    if not zip_path.exists():      \n",
        "        url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "        gdown.download(url, zip_path, quiet=True)\n",
        "\n",
        "    img_path = 'celeba_gan'\n",
        "    with ZipFile(zip_path, \"r\") as zipobj:\n",
        "        zipobj.extractall(img_path)\n",
        "\n",
        "    ## Build keras dataset object\n",
        "    dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "        img_path, label_mode=None, image_size=(64, 64), batch_size=32, seed=42\n",
        "    )\n",
        "    ## Preprocessing\n",
        "    dataset = dataset.map(lambda x: x / 255.0)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "\n",
        "    ds = ds.shuffle(10000, seed=42)\n",
        "\n",
        "    ds_size = tf.data.experimental.cardinality(ds).numpy()\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "dQdLVN4GNB6D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-LIsh-79yBC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afad7d63-d2b5-4bce-bb4e-00079bb7109a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "_dataset = get_celebA_dataset()\n",
        "## TODO: adapt code with train test\n",
        "train, valid, test = get_dataset_partitions_tf(_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0zHMM3jyBC7"
      },
      "source": [
        "Let's display a sample image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9b1RqfjdyBC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "3acb5c2f-db87-46c7-a4fb-6e8045fe5bfb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO19W68e6ZVWnb/zPtl7+9xtx9OddM6ddEeBiAGERgwQNGTEDChCCIkrQCBukPgNXOWCmwih5AKJq0FII6SAhCIyoJlAZiY9nXS6O912t+3tbW/v43f+6shFpms96/n2Vy4bd1JG67l6t9/6qt56q17Xs971rLXcoigcg8HQPHi/6gEYDIazYYvTYGgobHEaDA2FLU6DoaGwxWkwNBRBVefm5ubKrVzPk3Xtuq7qwx1gbPNx/PcqZFl25nUdx3HyPK91DrzWk4zD9/1a46o6f12smjc+J1636nd8jsLFNs8B3ice6NFx0sczk7qJjNGVa3uJPtJPpmV7rTVTfZ95Ya1sv3x9o2yvd/Sr2nYjGBS9pr6c08v1tT3n7Ofk0SkCeO5BoOfKW/FK8HxXvZtFLvP6z7/7zpkvjH05DYaGwhanwdBQVNJag+FZgOmeh9SYKKIfyCvpQefTmgrPM+zLaTA0FLY4DYaGwhanwdBQVNqcdUXxT+tKqXsOROX2NNs2K9w9T2K/4DmqtsoDsJV4jM/CzYLnXHKRVLhPVo0jozGGYQjXqun+qvl+8FE5/I5dVRGMw0WXDk9bA+M1qp4LI6/xGtiX02BoKGxxGgwNRSWtRapWpVhh6rOKTladg6HUGxXUsoo61KXUVaii0VX3ZgBUuFJY8eUHZ8tvPJe+I8/5dNfRtdmX02BoKGxxGgwNhS1Og6GhqO1K4UgIRFXkxirbkfvq2oRs2+G12T6sGnNd4DmrbOvnDx+3HE6FwKgenLWApjAKcL8CXC78fqhIFLLgihVtGtbTzwD+sljx71XH1bv48/x2GQz/X8MWp8HQUFTSWqRx6FZh1HVnPEnfKjcF/wapa5WbpSpgG6lx3eBqPj/S36pz1HXNVJkAPI5VfUuuKz+Atj7/arfTajpWFCndAQSf53h+fS0Pjot8PR8tf162A3g93SV3jAR258QRi0JURiGNH1VHVQokZXLR+HVsN/ww18/Fq/j2FTV4rX05DYaGwhanwdBQPJNg66YEwtaljDzeNBV6ViVar8LT7tyuouxVuZKq7rM6bw1Q3orbqqK1Suy/JIqHtqKMqxVCQbjaxKiaUzVv3Lei/STQ59BzWhQrvACcy6jKbKux9OzLaTA0FLY4DYaGwhanwdBQ1FYIPW3O2WeButdiF0bdSJG6tk1dVEXfVAWEV52jrj1d5fLKwZLKyY7Swdxnn/sXf8MYl8YB/1LA+Zb8FGKz+eTScVdENOG+gOM4TuCCDU7ndyGxbLbkSgFUuvngmeV8Drg3ZVqvliMtqctqvFb25TQYGgpbnAZDQ/FMXClPm2vo48YqWv5xU3Q+X92SEXXPyeerfZ/qv2Iak7tijBVzw3RPu4XAbUMJc7DyQ7jkSpG/1X2uFl1VIq+gtVVzlQEtTznhD9wAvvrVgR36FF76+HfCvpwGQ0Nhi9NgaChscRoMDUVtm/Npo0uqjnvWZfMYq87/tNeqm1u3yqbF/LCOszqKhO1KtGfYtkE3Q6WMEFRnHm/tu/AqgC2ZV0T6uK5+fQpHIkVcuJhHsrYQ/Ai9KNJ9cG00M5fGAQa0S/azm8t8FPSK4/ToaBNt1GZgnaZ0n5ev3Szb77x7q2x/8uVX1HFvv/1m2R4MWqqvHz3eiLYvp8HQUNjiNBgaimdCa582/8+zwLMYV91cQ1V0tSq3LiqX6lalrkJVUDlS3Krxcp9SV6nq1UTlwTVR5OQ6wDaOkaI4kIZyDiEPI3Mq8hBhOQl/KXUPuJ04fxGeH+eNo29AaTWZ6r5/+E//Vdn+1//yX5TtN995Wx33d//ePyjbv/d7/1n1/a3f/YbzONiX02BoKGxxGgwNRW3h+7Ogj88CT1KOAVE1xrqB0kxJq6gsYrFYlG0W5+OuLB4Xx7E6riqHEMKvqAaNf/M4VuU9qprvMOhQH4jR4bigImdTGOjda/W9KHBrlQMB4BeszPFgrgra9cbjYD6STAvrPchDlOZ6mfz0j39StjtdyFfUbavj/tPv/37Z/o2vf1P1vfuh7PJ+2Tkb9uU0GBoKW5wGQ0Nhi9NgaChqu1KehV1ZdY66kRtPUkZw1fmr3Ahsz62qXs19aCPyvczn85V9q/LdVtnBVUHlVfsEeC0OXm61RMGCKqaqHL+T8UL1pcVMfhdAALin7cowhHy0dP4olHFgwHOWrbYdOfJE/cVqKkhyhgHnBSmE4kzUTn67p/quXH+xbG9t7ZTtW+/+VB33j//JPyvb/+67/1H1/Z3f+k3ncbAvp8HQUNjiNBgaCrfKBbC1tVXLT/Esqm3VdZE8C5VRlYC9ivLytZHKrmo7zupK31XjqqKkH7daC+k703ykvK1I0z3Hl/ueL6Zlu020dqcrY/y1K1r4/ulLW2U7CoHmL+W+Xa0Qcn1Q/lCfX5xtPuUumQow5vDcZdV37ebny/Ybf/CHMl5yxwQtmbvYm6k+L+yW7X/zH94486HZl9NgaChscRoMDYUtToOhoXiMK6Xe2q2dv8p7AundCtMpo6RSlUU/AFV2VK8zkMvS6XCI0+lU9S3mcp40xTonLIX7fw9Ud8HGqrpjl+t1qGtJeylnFfyd5uJG4LOpvvmx6vM8sR+7rU242FwdN0vEBXMaa8nb6ULuc7srbhU30zabW6BLRNutRSb2YuHq3+Wr8gRT+T4s55fs31Zd7z18r2xvRvJehfwuopss0V3TGoFQ9uU0GBoKW5wGQ0PxTPLWVgMUK7TVXDeXrKJ+TMe8erQ2S2Wbv9vtqj7MlTqZaOqKVLZKxYS02fXqB1Sjm0VFlBD9mqF3hlVSK7p4ZnJnNZfCM6K6Z2l2VWVr3bXIQDGUoctF0/wp5Gw9GGnKuwuVrjsdoaudQI8kQtMhZ7cTVNgmPws+Qv3+VZXk0M898OBZw/ct47xPqHDiczpcFXwZ9uU0GBoKW5wGQ0NRSWtxV/NJgpU1jZNPfTvUl0M6WZfWZkSlckzjWCEq70IgbLzQVMpty7Wms5Huw9jfvIqKIBVcnZa/blXqnAOqce5IBI5lEXyVg0fDh+fEVcZ8UOP4cNNJqrcZ8SllzGvhgotM5jiLNa3d6K+V7aOJ3k2NYGe3tyY0+cqW3pH1XUgHSruk+M4V/MIAga8yq/Q7vZoa47VcOq6qktjK6tj4+8ceYTAYfiWwxWkwNBS2OA2GhuKpXSl1oyuQh3MqJ2WJsPmia7WVzYzL6+HWflWAMvTF5NI5ORKlS75kV65OiuV5Z9vFQcDBxfJ3kmgbjoOey3MvRWFAqQMyZpSdCb8Ll2xfaXd7OqIEXUEqwqbCjl/QM0vg76yQ+0rJNj05GZftAaizHMdxDueTsn3nYCjH9bfUcVFb7m0pRRhWxK5Ur4GtThW21W3TfbroHlxxPsdZzoVbceiZsC+nwdBQ2OI0GBqKSlpbW8FDv0O65yF55e3jYuUfKwXtHlG6tMLdg3+fDk/LNlNLz5dp8Gg7HCkp5pV1HMeJInHP6IBqTX/RHcGKJhXMDcoWn/IVtQNUrZNiJZNz9gJxOQzaOq9sKxIheZxoOo1us9amUMjpTLs65jAHk7l2SS1AB7NQ80g5coGIpsSNs0jm8d4DeWYXNjR57W3BvdEnBmln3cAIZr/4KrHbQ1doQzcfV0JbzV394vEWpX05DYaGwhanwdBQ2OI0GBqK6lopYIsts+d8RVtzbSwZR/GmjrOiPofjOE4G56ySv+VgD6BN5TiOc3ByIueDYOioo90IeSz2V4tsvTiWUbda2obb2jpXticziV4JyG6dLMCWpFvugSxvG9wKO+tr+ri+3NtaRwcor3ckyma9K2MsEm0j+zB3PYrMyaAuyb2Do7K9++iROm68kLk6GY1V3/FErvdoLsdlPrmWIKIki/UYOy25N7SD7+wO1XEbGIjd4crZ8rugrreQg+xVHxu1K95bCthGk3NJ4lrDl2JfToOhobDFaTA0FL+EYGukpJz3VagPB7QWihLIOaJIRydEEMg7n2mKFPhybAbujITcCF6A0RoavZ7Qy4A4Kab273SETs5G2v1wYSDn2GrpKX8R6Otrr3yqbF/e0oqYHgQNc1mIVktoY55hZAu5jIC65TTfs4Wogi4O+mX7E+f0OO7e3yvbtyiQ2YV8OqcH4gZZkEvBhcDpLNPnmIKrZr0nFPdgqKOFDk7luF6kzQ1fRTHVq1peBY42YcXTKlTlQ66TO8q+nAZDQ2GL02BoKGxxGgwNxcdic6rMBfDvOUfw10x8hQmcPF8f1w3Frjyd6619D/ow72tO0fFRG/Ocqi6n3Ra75xzZgS6cJwYp2/aWtoE2YZa//KKuu/HqJ6Sc3M6GnJ9tcL+12u5GY7KAbX9OOJWADRrnup5Lpw1Jq6AddPQrAlPqhH09jvaxuDtGkG3iAZUKDELcC1BdTgrPel6IjR/42n30zoeHZXt74zqdX9puoa+Ntl9VHRwE97ANugpVLkCzOQ2G5xi2OA2GhuKZ0NrlT/TZn2yXyxQgrWXlD1AHlduVApkXoL7p97WqZjgVlwa6VTa2NtVxMwjw7ba1ymgd3CB9X9O46VRodA9Kxp0vtAvjr70qJeO+9OIV1edCPl28z3Cgg5C9NqiMqKQeegsKiD5nHUsALg2XkpxlLowjh3G0NYXrbsL5Cy4PCFWvwU2W7x+p4+aYO5aSvo2BbaeF3Ge7pxVN8+lB2d7d02Uh1m9sy/kzTd+RamKge2XyAPqGeTVzJVfBaK3B8BzDFqfB0FBU0lpU9PBXXwejri4P4GJfVbA1nQNT3kcgaPdcPeTRVHYIeXeyDaodD87XijQt3OifL9tclXp4KuefHGp6thYIjdveEPr7W1/5qjruwkAoGef1ibpQ4Qzy+gS9vqMhwnpmRFhyAPPP8v+9aSrz79EDzYBSQ4yAE1BOJTeBe+5qMyKB3etfuwQUlxL5fHggO61hiwKxQeyOZRUW89UJix4e6xIaO1tiVmx3abcZqm8rwVdOpgL+4a4OtnhauDWCwO3LaTA0FLY4DYaGwhanwdBQVAdbY35OtnOwRgQFo6qTKr5OpfEwJyy5WTLY5t6+fLVs7+3tqeOiFcG5juM4fbDb0E3RjrgSsozx4d6+6sNd81aq1SYvXhRFzzf+8lfK9oVAn78DScJQHeM4jhNCEi4vwuMcDU+O4/y5bCevgg9291LJFsgzmwfouqL/v8G+DQs9DkwuthbImK5sbajjjibigno41vZiC+x4DNpvhXoc7bac82Sk9wKmuexRzBbarRVAnD2+wywNQ1feUtXrFaUUqxLMLSF//HfRvpwGQ0Nhi9NgaCiqFUIFbmtrHuRCH6/wXCko5HecR4XpGeLiFVHSHB+LAgSF6I6j88pyaQPsqxIh378veXI80tV4C6FgVze1aufv//XfKNvn4J7bFAwdAFXLKPA4A4qENIv0/U7mAR2mecSUNjinTPN9yPmbeVQWwpVjka3m9IakvszxfKrpdAQDGcDcXyDl1i24lyDVgekFlEXA3Lo5mSIL6HNJMXV3T57n4CK/L1AWEtRJxRLNVCWwVQ+788pzkO3HyQX0wVYC0GB4bmGL02BoKGxxGgwNRW2b0yM+7Xtow2lbj4OqPwJXu0PTjAOI0T1QlSgJwfYo/m46lS378VgHZWP8Bud67XhyL7/++quqb70jW/YbbZHouWQTenBvBUVhuF2otwJRL16oo2NaPbF3lxKUgWGItVgicoMU8AB8X9s8uSfzvSikndG+QAbuErfF/jX5XQeSfUXk6tmESJ/397QbpAuB6nP4XUr2MwaVF4W+z2MI7o5zLYNcQMC576CNv1TnD87P+wT1UGVzuhV1VD6CfTkNhobCFqfB0FBUK4SQSnDkCSh6fK4onaOyCPL/0HG41R+ScmYykQBoLIPQamm6py5LLpL9fVH7zCHHD7pYHMdxQg9KBxQT1XfjRVEnXbt2UfUlcJ+PRkKbQ9rab/UhaLir8wu120C7gNb6oT4ugUcRtXXgsRfI+FV5Q5rvAO6bgjCcjlL7QCVuyn2bwnGpo82DHHIDB5CfN4w1NW6B+ZGRugzfOR/LTFIJhACib2Ia4zyRZz0nU6rvyr2pnLbEVV10pZBfixVxq1AVT12nMqF9OQ2GhsIWp8HQUFTS2gwojM+7pIrW6i4UCmPArEd0AHPhcJ4W3HkNQAUeRXrIJ8cSuHt8qCtRYYC1j2J0j0oRwK5gJ9f06fVXvlC2pw91rpp7Bz8r2znMTxisq+N83KGleWy1ZFw7Oztl++qVS+q4K+ckL05MeY66EOjdwjmONadLY6F7s1iXN9h7KKqak4lQ9IdHejf16FR+N0m0CTCHnEptTH/JuaNyoaHrIaVEbcux3gzSmS70tbwI1Vr63Ylh9/o+lIVwHMfZGEiwgueLOqkodE6lAt6drNA2AFbO9lW+KA7sEKT03AMLtjYYnl/Y4jQYGgpbnAZDQ1Fpc6LSZTmQFAJyyejE0nge2Bt8DnRpdKnScpKIvdHvi7vh9gfvq+MmkHO2HXFSLAyYhRKAtPXuQ6Dx9Uva1lsM5fyXXvqE6nvtC6IYasEYWe2EiqTjwwPVN5uIDTccik17vKfH6ILNdeGyLunQjmX+E7Azk6mO+JjP5e/j0xPV9/P33oY+sd0TUrmE4MrCxGWO4zitLbEDT47E1nt0ou3byRFcOyWlEuTdxXdHuYgcHZzvUiQRJnobUlnIKfhWOi1w2zgcdXV2dfY/H7T0wXFBhYvFczihGkfTn/Ubg8HQSNjiNBgaimpaCzT0SfKjcJDvR2BhOtI/FKb/4m9xb0xga39CVK0DImrP17ejxPNAtX3KCdMBOvLStWuq7+b162V7Eevf7e6JGyfxhcaxyqjblvvskDpp0Jc52dnEnEeU0wYCpWcz7R4YxEI1sdj0LNPugSlUYYtj3bcFeX52Lkge3xOqKD0At01GkQwYZL69LiUvXiD31AaUZ9j7/g9U3wiCJvB9cSnA2cfgc0cL61ttSRQ0TfT455BbN4Fvk1+Rm3bJ6wGi9QzVcPTap4WMOaGo9eFSXqJl2JfTYGgobHEaDA2FLU6DoaGoXQKQlfhV9ihGn2Cbg1bRxcC5V7tQh2OxkO3wrQ2qLg3XnnKZZLheG7beW2S/3NwRadwrN66rvsVM7N2jsbZ3T1ORvN07EfdAOtd2zvVrEtly47KObHnts6+U7WQktuR8pm1wtONHR9rmXO+IjdWGPLizmbYrh0MZ48GjR6rvFI7trIn9+cqXvqKO60Bpwv/zoz/R55/JMzzce1i2I4rEwVxaGxs6+dcxRCMFILmMF3rux2O06+ndhHduONUumGPYv+iBjDPsaLs4Vwns9PuSpWCrgktkSu6SQwj6PhjpZ7F3zAH/y7Avp8HQUNjiNBgaiupga0VDKXpABfLqvlTlqoGgWKKuYSAugPPnz6s+rFiNW+odqi6NeWbyhd6eRpIRQE7Yi+s6auRLnxRq2SV1zwzUMm99cFv1vXX/ftnefvmlsp1mmiJ9/pooiy7f1Cqj//5HPyrbX/3CZ8v2pRe1CuitH/9x2R60NE2cnAjFS6AC9smxjtI5OhTXz+GhprVXb75cth+OhI5969vfUcelvjyz1prO43v3fVFvtcE1sXNemyKDc+JmuXhV0/yf/+Tdst3ryXPPc03lMY9SQXQSg+5zT7/ix0Ohk+c78qyxfKHj6BJ9S1FXqZxzCh6YXZrvO4dyrQcj7apZ4W1UsC+nwdBQ2OI0GBqKSlrrA13IqTpxEcjfcazFxSBmUWLgrS1Nb1qR0DNWgGQQDJy5suM2zfUuZgzC6YB2a71Axr8GO4av3XxJHfe5G7KbukaUN9kRur11+YrqW/uzN2WMQIOmm5oau7GMud/VfV98VajsIxDFHx0+UMe1ISibK3hjsHgC830y1kqlYSzzmHd7qu+d3Xtl+9XX/2LZvnZfV13L4Dl99gs6Vei1b36zbP/bb32rbN99oM9xFVKKtgaaorugYkpnkFKUcggVEMydpBQkAK9SOtNzNU/kd6OZPLP1NlWGw9ShlPp1BuL8vZH0vb+v383hBEo/ULz5eSrtcRbsy2kwNBS2OA2GhsIWp8HQUNSOSikKUu2D3cO5ZPMMA7GFbIdUigBVLxxMi0CFUEDJrSLY5/ap9F4OtupnXrpZtm/euK6O67VFpbLW1VWYo0gCiq9c0td+5eVPl+0EIlYyCtzFCJOep20bbwA27UDs3YDOMdsTt83wWAdKZ2BzYjkGN9dzGkFZ52svfFL1Yem9+anYTl97TduVBSitPrx3X/XtQymL3/4bv1m2FxTcHvXFznwECdocx3F+9nNxVy1ASlTQfgI6T3yK4Elhf4RLIuC7NJpAkPpAP9sQ5iNJ9DlOJnKOBw8lwmY60ccN2jKuC+f0XsYL5/XfZ8G+nAZDQ2GL02BoKGoL330KZEbVTk5b2SqvClBNpBSOwwGzTE2gcjHQ63ak6cd0LBSvz7QZtsCR1vboHD1QKkX0/xVWP47nWrycohsHxjigoHKEt9AqqRQE1vM5lHSg6l4zEGw75ErRtA5y5HRIHTMS18p4qIXXa2tC7TdA+bOUFgcCjS9+SrukMCoZRfdcNHoBJpJL97K5IWbFrbt7ZTsIKAdPIvMYhuSnAF9eTi6YFN7VNJNzJoU+LoF7Gaearu4eSq6n/VOsgKeHsQ3ukstb2nXVd7WQ/yzYl9NgaChscRoMDYUtToOhoajtSvHc1cr/gm0gsJfQzcLb2mibZtnqczhYto2SSmENFzfRNuEabGX3IK9WSKXx8lRs4TzWfbGLJel03wBsnRRsoMVYB1tj/lWs++I4jhNF8ncHJHonR7ouSzyX87foHAFk9fLA3vJcHfrQ7cpxwxPtBoEAECedyu8isn1VWUiSXHq+jGsAv+M6IQU864ASXQ1AZpljdBPZcym4Z3KqfRPP5XnyM8Phx7HMVZzq49D9dUq5b0+nMi5UtW6t6+eyNZBJ7QVcRtB5LOzLaTA0FLY4DYaG4jGulHpVqTnVPOZ+wYrVCZWkm4JrZTrX1KENSqAUFDA+KYn6cAd/6YtfVH3HxxJQ7EMwbV4RHJ7GOrLAy8D1Qf+V+S0oTRhIZ9TTrhQftvNDckkVQPXThVw7j/VWO1ZyZhPDhfkPfDxOU1IsmxGF+mZmE3FJ9VqYA0rfiwpszrVbqACzAulewm4yaGfkWsL5wGgnv6VLPxTwMGJ6rwIwI+KUAvxhDlx4/ckZ4xQFKKYSUmuBawW8McpEcRzH6QSo1tImV2LlGAyG5xe2OA2GhqKa1qKSg9LVe0AElnYxe6I2SYFyTCaaqk1BKJ1T2sz5QmgA0sIg0zTlK58WlYo31Hlm/tFvf6NsvwdqkyTRO4QLEMhHRN9RdNTtk+geqHfYFppStHUAMe4wM60NgPJlQElPqaL0AtROIeUQ6kJqzAB2fPukbNk/EFVQ4Gha1W0Jfe1CMDSbMwGoqdK5NjHSVGh5AuUe4lgfh4qb+YLPIfeJtNwP9T0nQIe5/Aey/pQE8xlWOIfze3wcUOMZvy/oZQAFXESBFy3gyjlvz1pla4Ph+YUtToOhobDFaTA0FLXz1nLO2UFfFPcR2VFYzg9tiMlEJ5wqwP2w4HIMHbExMBB7jYKQv/a5z5XtdarB1geVxyYk1hqPtJ0TQ6D0wiNVDbhFPFdv5weu9J3blKDp6JyOQDgB+3F0SMofyL+agwqoSLQNtNnRZQsQHUiUdu68lJZgG3+9LSUGc0paNToRe30DKlZvn99Rx/X7EiS8IJtzcirnGKLii0onLiD6ZkYutAW4T9BWXaOkaTn8rt3R9ugYylpwTWqMfsISI1xSJAU7ll01+GhQndUK2CEj58ipJ18e2BLsy2kwNBS2OA2GhqJ2sDULtlX+H/pGt6CkAVYS80lEHYPoOa8Qz8+hmnV7ra+OwxyuN27eoPMLLapiERmKqAuaEtjyTohqdkDlMYYSBvlcp+XfeyA5aAOiT1vrElycAc2KKCB8ATQupm3/0Vzm59GpKH0e7unct+gWCYmCYVByCtcan2j3lAdVqn0aYwDmTQvcPe5UmywelEgoSGWE71WvJ+bBxYsX1HEjeCdSUgEhXWWmqYI5sAIevSH4Pib0buLrHsDvAofLgcC3j3JHFTV4rX05DYaGwhanwdBQ2OI0GBqKSpsT+X+ntTppFQdb7+2JVC4C+5NZdpafbV84juNMwB2D/H/3SOc5fe+R1Be5uH1O9X3281La78cf7pbteKptg60I/4/S9twColQiivgdolxwInbm9FTXBnHBft44r8eIbqIY3BvtdV1Lww8g2Jpq0rmRPMbhUMZx4aqu7TKCfLccvDzotaBPnlRBNXIwZ64X6ncCk6+hvcXJ4XKw4ynG3okgigmf+/6+LlkYQ3B7QS40DJROWdoH50dXIb/DKQwspTnAM4I3cCkJGcpf+T7ZtXIW7MtpMDQUtjgNhoaikta2gAKklLtHbTVTAHQAQacpkllPE9su5HdFVRGfH/8LSSkC5vs/lMrQI8rF+t/e+LOy/dZPf1K2/+qXv6KOuzCQbXq30Oefg8qmTREaLQiAfgR0+/wlrarBYOjpWAfdepjjF0tXUNQCqqm8LpUOgPIG0QDzCWtKN5kBDac+DBqOYepnlGvYg9J43ky7MNrgt8AnvaBSHlOgmjN6nqen+B7IWcZjbc7kuTwXpMKOo3PTekw1Y8wNLKZDGmj11zyDvLtUhRqrA0YwRj+gsiQYuUW8lqO8zoJ9OQ2GhsIWp8HQUNRWCLEwGGlnQEGmKuUlUF5UDjmO44xGkkKS1UN4PbxWTBzjYCZi+u/9rz9QfV0QcONOKwd2p6D86W3q6k85BH23qMyC2m3uS19MND9HNRUJ/FFu4sMYI8ox03UgAJp2D9tAgVsuqLNI3dNZyHOa0TgSuJcFCr2pglevL7vqXodKY4BovY0qsZmm8njtjLbwMT48hefOx+GzyGaIfh8AABGFSURBVLhCHTxf39PvlYc7qKCKWuT6HZ5DDYkZKcNgM1u9pxmpflA555J6KDdaazA8v7DFaTA0FLY4DYaGotLm5ABrBCp/OMES2ohYKjClCsdYqmFO5fU4SqX8TU8H1o4KOWevr7fDT2BLvQ9mA7tcTny5l8uXLqo+H9w/RajtL0zwlUNq/9lUBzm/CW6c7W0dXYGJn3yI/BkMdHD1DPK2nhzoytb5gQRwt8GtMD3R0TELyIs7i3XgewYqmzHsBUwnurTEhYsyP5de0HOFicEwHGROUSMJGG05qZ0eYmIzyK42pzy+vb5EJ50e6+fZhpy8OSXnQjfRGLqyU+3Km6fyPs7J4CXPSomlnF1or1Mn5x4+C/blNBgaClucBkNDUUlrMRi139dBziiw7lAOF6S5SFcTyjmL1JXzo7Lr5iNEHg05RxeJ/k0C2+Z45b39A3Xc5U2hkEmuqffGmtDJgip4h1C6rAAvUaen6fXrr71eto8ONSXd3BAh/PXrUn378tUX1HFzoPM5qdYxV62qHk6Ubj6WZ5YkmsbduvV+2f7ZT0VZFU30vXQ25T04f0lTdA/pKrq/KDi8A+6Y3Yf6WUzAlAq74C7J9T1jvtu1gQ6amEIFb/KkOBlURru7L5S9cPVzUUIxsrCQrc4wL/NMz+l6V+bKp7APN1lFjuE6jz3CYDD8SmCL02BoKGxxGgwNRaXNub4uUjZ2l+zsSOQF56OdzTD5knByl6RgKNkrSFK3yubskvJ/CgmtfIrWcMHg6PaE/59QTZXJVMZ/cKSDerfWr8sY6f+yOcjV0O7L5lxRGnLf5jrYen9XrvfwliTk8pw/VMft7Ih9t33tsuprrUM+2mJ14qjJXFwOu4/uqr7b74vNOR3JnK6v6f2EG9uSn9clOwoldbu7EtzOWbZwa+Du/V3Vh0nO5hOx4XySfgbg/nI5oBpz07JHDt65HN6PtGAJoPzNryJ6f7C4N18K10xBla3NlWIwPMewxWkwNBTVUSne2YHAjuM4WSF0ldUb6DIpgFe4FFiLu+NMCQLgPm3IQbOxpnPrZECh81yPsdWW320DRQ9OterFD4TCHJ7orf39B5JX9sK5bdWXgSooAOoWUkmHZCFz1WnpaJOLO0ITJydCr1MqYRAD9R4+fKj6NqCUYrKQ+Rge6ADlh4+EQh9ONLV3oCr1FkTz7OxoFVCeI6ejEoOnolTah1IN80i7Oh6AQutHb/xU9XmhmCYFzgEpxnAe3UI/9/FIxpWQKyUI5Fn4kC+qoOBznSNAU9AWUNI+dK2Rmy+EIbuZPkfhPf67aF9Og6GhsMVpMDQUtYXvrAJChRDv1uJOK+7CVu1P8e4sVstGldHv/M7vquP+/be/XbZzos3TRCje3/zbXy/bD/7of+trA/0IaRwH+5Lms0u7jp2u0L8EhNLtDimVsKIX06y+PILN3mbZLqgqNZadmJEZsZhAvhuoVDaNNXUt2kLd1tta+eNGYi54kGoz9ymAGP6eD/Vz//CBpATNu0JlP3ykafh/+cH/lLFTXilMZYk5rCIyqxIoGXHu3Kbqe3QgpknF5vVKjwCDSyf4kDuq05ExdshbgEPm9J2FVzGwP4d9OQ2GhsIWp8HQUNjiNBgaitoJvlD14zja5uSIEkRtXk/GQe6cHbHyvf/6Pf1DsANPTrWN5bbld9/57nfK9sWFVoN8/Wt/oWyfH+gEX+MjUcvcfu8d1XfjpkSROAEohPQInRCCqDOP8peC7ZFDFE3Ypm15UEYFhXbHoEsjW4A9tKaPyyE6ZDanCCGIdDmGoOy5qyNKTiD4+v49raZy22JnvguKo//xxp+q4+5Cde82leHwUnlmrVDGH7DxiHmUOWEbPAAOYkqhE1/NqteUtjKU0i2EwO4g0AcW4KrhJVKl5Cp/89gjDAbDrwS2OA2GhqK2KwUrVH8cYFZR5Ge7YO7c1YLtFKpNRS29lZ2AUmcC1Z9jylEagX8jI/r+iReulu3RSM/BBx/cKtub25fKtq/jdp3NTVEZ+R19bQ9U1J5y1Xh0nPyu29Z01YffpZBnN5no4N8p5I/1yd0zgvteAI8bLTT93TsUKlv4er7fvXW7bP/gzR+X7cO5LumgynXEmjYXHip/5B1o+/qeX/3Sl8v2nf17qo8DrJ89xPzIId9vRsH4vp9Dm1xjRmsNhucXtjgNhobCFqfB0FBU2pxYy4RdInVdJFVwK/4qICwArzWhUoEb65Kca042YQzJrnqQczaItFGSojyL7iuC353f0rlkr0ISrp9/KHbP/r42OuOp2CKXLl9SfT2oKB1A5K5L89HrybU9khFmIA9UJe90fLITQlKpkxGVXAQ3wNGx9O0eaHfJEcz/m+/cVn3v3pM5iEECGLT1QCIIlI5n2h51YPyqyjW5foYnMsfvvfO+6vPBPk1zqnOC4SaVZh/OPycCkHkM4V3yKa+xC9dOKMoo8x9vGNuX02BoKGxxGgwNRe28tVXg0glcEvAjLOUJKlb3ITBO1eccqEB9jmkruwNMYgDHbZ7XQdOjTKjVVQqGxhxIg46mZ64rrolPv3KtbE+u31DHfXD7g7L9zvtaZXTpggQzX7kkuYFaHV1ucAEB2/lsdf5fLHGREa9NYCK5DMKdEwnMfvu2uIj27u2r4xIokbBPZS2CSMa8sS5RLuOFLrVRgBopDfU4fJhjzNF048oVddx4KudMWTwE8+GSRKgFLo0IWShF+8+gbgOvggBUXX3IHdV2NVUNfcgdRSy28B9vFtqX02BoKGxxGgwNhS1Og6GhqLQ5q+xABJeMDyGaAHN3cu7bpwFHpU/HEo3fa2k7LYfaFRcgMdgFqvsSgcGRJmQfpXLO0NXZIDABaxvz6Q70tH7+C58u20OKnDk+FFvv1u135XyUeaIAe4bnG+3iDtiq/PxOwNV0f6zH8cO3pD7KIWQ0eP3Lr6njphDR/6e3tSslBHs3gHegH+r5GEGysnCpLDy4UiI53wf37qjjMFNGSq6OHCJPupE2Jjche8XWQN6DOckU96GmTUYlAItc7g2VibOQXFxg71LKZsfLzOY0GJ5b2OI0GBqKSlrL9KkuMJpFJfh6AlXRKkrN+T4nwCvW1/p0sNCbqzviPmkV2h2TFUJl41xHpaS50Ms401EYqmo30rEWUTVQm3QpCVTgS3mGoCKcIgHqOqcoDxciOcZAXU+JQp9O5N4miZ6DjZ7QfnddXFLrXR0Mffv2e2Ubyy84juOEEDg9hCimpfcog8rnoXb3dKBsxgISu+WkigqhPEMy1RW8UdzjkZ8lhmR0J6nMY0wl+TAYekqvYgIZ4QpQ0T0aa2VYgGUhuMSl83jYl9NgaChscRoMDUUlra27u8pKIlYM1UHdnWHe5FqAzOh0ofOotoC2nNveKtvR8EifBERBMVWbmuZCrboO5btBjbmH/045SuHeOl2tQApAsZIR1US4qD4pKGAb5r/Tkhyu/Z6mnW0Qi8eHulTDHCqvnV+Tc0ymekx3Hsjvci7RAe01KH+BAQiOo+djradNkeNToag+BGWHLU1/56gUW0rQA0HOtE3aBZoegvmRzmmX3oEqY7l+ni3I9dSD8hEdVz8XvM+MSjpgvqhVsC+nwdBQ2OI0GBoKW5wGQ0NR7UoBFQbbkTmoMlihgXk9kWoHGdmm8LuCNpe9FfVWuLr0LIVkS1SSDo3CW1BB+dc/80l12P1dCdbttLUy59Km2DpcJQ6jH1qQmzZkOUiO49fA+cGaKrz1jvYo2kOO4ziLWP5GUyzP9TjQBYMVwR3HcfyJuD7OXZSA8KGj7cVdqHrtDTZUXwLuiKKQ8YcRKavAft4/0ONY3xLXEgaRJ3Pt4sJglpbP7xWWneRq6pBnFmxmLnGZQ6QSFaV2uvBs1kAZ1o/0foJbYVcW/K6eAftyGgwNhS1Og6GhqKS1OdbGo2XsVWgckEqkQLk6RB0iUJgUdP5TELTjDnW+NBC5VuKsVub87I7ku71EwdY55HcZTzV9mkCu142u3vbPIRi4SERkX3iaxqFCZk5b9hnwUNxez6hkhKMCCHRfApWtC3CzzKb6uBncC+fu8YCiezDeg5NjddwI1F/hVN9LBMHWMTz3dpuVT1BygYLK4wR/J5RxQe6YHCgvsVqnSMCM4BKGcP4ClGJZql1GOOKCSzpAewHPxYvJvMMIbldTXI9NnzNgX06DoaGwxWkwNBS2OA2GhqLS5kxhuzcgjoxBsbzCXXAdrPckuHWbAoh7EMWQ0LZzCmXoUKI3z2h7GoyDnINuIT/qEdhAP/jJW+q4T10Uudrmlr6bMdiVxyNtf128IOd3Pdj2p6gXDP4NIz3laD92wMbK6T5TyNvKc5XD1n4O+saEAohdOOeCasJsnBN5ow97AYe7e+q4qCXPM6coGoxEWQf53pTsbHTLYXTJLwBB1GAHpmQTog3HCeWSOUjv+OwwV+hWKRJtn6P7jh0iMbwiExgXq109JWiks9QIS7Evp8HQUNjiNBgaikpa28bydKRYKSoCZjEKIYESfZ3t8+q4c+tCJ4cTnQO1DflXCqgGvSDugAGtGYWs4PY7ViA+piiDnx8IXQ2p0vLFbXG7bEU6UHoMboUOTE/IW+9AaxMqJ4GUFyrGnRHojmWYKXAX5iqB4OKIaCd6FTYG5BYCejkHV9j9RzqCZ7qQ8bYp5yyqnaag6OlTziZ8Li75QXzwmxUQLN+i8o7jiUSv5FxyAU7J84jvsVKecWV1eBYUUOK4MEYs/eCzUs5BlRtVNF8iy8uwL6fB0FDY4jQYGopKWrsFgam8W7aAoORFrHfjwi7k1gHB9p2DA3VcChRsndQ325s7ZXv3VH7XpjT2RYK7xiRednEnTdpDElE7nlz77ft6jB6Iudde/7zq80M5Zx9ofp5QcDGcg0XxSSrUeAGqnVaoaVyaIA3SlEjtLFYEuuPUdYiipz3ZSd8H6n00HKnjwkCOO6W+AGiuD9x+PNVB8KggCyt2LTHYn2kn0lO+ZTyUAwjC8GzaXAWPxPMtyNnUhureEVFXH1VBLHTPH39t+3IaDA2FLU6DoaGwxWkwNBSVNufVi1KS7vhYq2PmsB0eU+m9jXUJwj06kq14dsfsTyTQdlhoNctgUyo5uzPYuiblP+YXTXJSxMCWtwfl8Nxc26bjIShKUh0we7cvY/7+m7dU3xdfENfQGrgLot6mOs6Dvf0p5Vh1UWESS3tBJQASyK0bFGRbY64rDJB3KD9vKHbOAEr0OY7jLArZJxhCbtchRcB4EFDskR2Ff1YmKwO7O6cIdhdUUk4u97JIVpc99LlGOlTpdj09fteT6+Wg4EnJXsSrBeQiaaGrBs7he2QXw95AwX1WAtBgeH5hi9NgaCgqae3woVSbikm83O+LmyUjlUcb6OtVUNicnOh8MRhYm5MQewi0uQ8Vq2JSxwxT2fYPaMtbERWgLSEpmhLIb5tROYbd/ftle3ysr72xJm6F4taHZfurn9EqIxfcJcspVpHKynEBKaFSOEfLp7y1EKk+hcpqc6oojQJxP9T0vXCE1t65L/dckMoIVS8+PYtVro8ldVlFiQ4cYwJBB6z0ydKzc0w5DrmTigqlVXG2WugX45I2j3+VeL6KqeZOPbeNus4T/8JgMPxSYIvTYGgobHEaDA1Fpc35V7761bK9oGBUrF0xi7Vc7dYtcTlcuXKlbLvXr6vjdsG2STO99X7hkuROfXQokrqYKlu/+8EHMiayo6aQp9UF24mrV29vidvmZKRdHTMcFtlfP3z3nbI9TF4o2/2WntZXPweVrY8eqT43l3lNwCW1oEiLDAKn55S31lVJwuTfQypFGEGO1Vak5ZKjI3GffLgHAdZkSKEMkjOvos2JtiRLCqvkhlhW8fjo8MzfOI52GbE1h4d69MxQBpmCfK9KAshRQBjUn6ErryLyJKXnWYSryz1+BPtyGgwNhS1Og6GhcOuW3jMYDL9c2JfTYGgobHEaDA2FLU6DoaGwxWkwNBS2OA2GhsIWp8HQUPxfLl2t74d/i90AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for x in train:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architectures"
      ],
      "metadata": {
        "id": "pOeojm615FW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DCGAN"
      ],
      "metadata": {
        "id": "o1fAbtpW5PID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.discriminator = self.__get_discriminator()\n",
        "        self.generator = self.__get_generator()\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    def get_generator(self):\n",
        "        return self.generator\n",
        "\n",
        "    def __get_discriminator(self):\n",
        "        return keras.Sequential(\n",
        "                [\n",
        "                    keras.Input(shape=(64, 64, 3)),\n",
        "                    layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                    layers.LeakyReLU(alpha=0.2),\n",
        "                    layers.Flatten(),\n",
        "                    layers.Dropout(0.2),\n",
        "                    layers.Dense(1, activation=\"sigmoid\"),\n",
        "                ],\n",
        "                name=\"discriminator\",\n",
        "            )\n",
        "    def __get_generator(self):\n",
        "        return keras.Sequential(\n",
        "            [\n",
        "                keras.Input(shape=(self.latent_dim,)),\n",
        "                layers.Dense(8 * 8 * 128),\n",
        "                layers.Reshape((8, 8, 128)),\n",
        "                layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "                layers.LeakyReLU(alpha=0.2),\n",
        "                layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "            ],\n",
        "            name=\"generator\",\n",
        "        )\n",
        "    \n",
        "    def generate_samples(self, num_img):\n",
        "        \"\"\"\n",
        "        Generates num_img images.\n",
        "            Returns:\n",
        "                imgs: an array of PIL images\n",
        "        \"\"\"\n",
        "        random_latent_vectors = tf.random.normal(shape=(num_img, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imgs = [ keras.preprocessing.image.array_to_img(generated_image) for generated_image in generated_images]\n",
        "        return imgs\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "u2mfR4Hk5cWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE\n"
      ],
      "metadata": {
        "id": "JYsq8qxt7y0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "u2Hp2RCkK6cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, latent_dim, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = self.__get_encoder()\n",
        "        self.decoder = self.__get_decoder()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "    \n",
        "    def get_generator(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def __get_encoder(self):\n",
        "        encoder_inputs = keras.Input(shape=(64, 64, 3)) # avant 28, 28, 1\n",
        "        x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(self.latent_dim, activation=\"relu\")(x)\n",
        "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
        "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def __get_decoder(self):\n",
        "        latent_inputs = keras.Input(shape=(self.latent_dim,))\n",
        "        x = layers.Dense(16 * 16 * 64, activation=\"relu\")(latent_inputs) # avant: 7 * 7 * 64\n",
        "        x = layers.Reshape((16, 16, 64))(x) # avant: 7,7,64\n",
        "        x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(rate=0.25)(x)\n",
        "        x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(rate=0.25)(x)\n",
        "        decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "        return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "    \n",
        "    def predict(self, input):\n",
        "        z_mean, z_log, z = self.encoder.predict(input)\n",
        "        return self.decoder.predict(z)"
      ],
      "metadata": {
        "id": "BeisHTws72i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeOQ2YkyBDC"
      },
      "source": [
        "## Training code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "d1t400Il7-e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejwQuN9HyBDB"
      },
      "outputs": [],
      "source": [
        "class SaveImagesCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, dir_path, num_img=3, latent_dim=128, epoch_offset=0):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.epoch_offset = epoch_offset\n",
        "        self.dir_path = dir_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch_adjusted = self.epoch_offset+epoch\n",
        "        if (epoch_adjusted % 10) == 0:\n",
        "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "            generator = self.model.get_generator()\n",
        "            generated_images = generator(random_latent_vectors)\n",
        "            generated_images *= 255\n",
        "            generated_images.numpy()\n",
        "            \n",
        "            dir_path = self.dir_path/f'e:{epoch_adjusted}'\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "            crnt_timestamp = str(time.time()).split('.')[0]\n",
        "            for i in range(self.num_img):\n",
        "                img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "                img.save(dir_path/f'generated_img_e:{epoch_adjusted}_{i}_t:{crnt_timestamp}.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "zFJdp0zi8Ah9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_GAN(latent_dim, load_weights= False, weights_path=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        latent_dim: The size of the latent dim\n",
        "        load_weights: When set to true, loads weights\n",
        "        weights_path: The path to the weights\n",
        "    Returns:\n",
        "        gan\n",
        "    \"\"\"\n",
        "    gan = GAN(latent_dim)\n",
        "    gan.compile(\n",
        "        d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss_fn=keras.losses.BinaryCrossentropy(),\n",
        "    )\n",
        "    if load_weights:\n",
        "        gan.load_weights(weights_path)\n",
        "    return gan\n",
        "\n",
        "def get_VAE(latent_dim, load_weights= False, weights_path=None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        latent_dim: The size of the latent dim\n",
        "        load_weights: When set to true, loads weights\n",
        "        weights_path: The path to the weights\n",
        "    Returns:\n",
        "        vae\n",
        "    \"\"\"\n",
        "    R_LOSS_FACTOR = 10000\n",
        "    vae = VAE(latent_dim)\n",
        "    vae.compile(keras.optimizers.Adam(learning_rate=0.0005), R_LOSS_FACTOR, metrics=[\"val_loss\"])\n",
        "    if load_weights:\n",
        "        vae.load_weights(weights_path)\n",
        "    return vae"
      ],
      "metadata": {
        "id": "sM46ZeXR-FTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model configuration\n",
        "load_weights = False\n",
        "weights_path = None\n",
        "latent_dim = 128\n",
        "\n",
        "## Training specifics\n",
        "epochs = 30 # In practice, use ~100 epochs"
      ],
      "metadata": {
        "id": "NMKrW8sBVBW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "if model_name == Model_names.GAN:\n",
        "    model = get_GAN(latent_dim, load_weights, weights_path)\n",
        "else:\n",
        "    model = get_VAE(latent_dim, load_weights, weights_path)"
      ],
      "metadata": {
        "id": "xl0sehedAMtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_images_callback = SaveImagesCallback(dir_path=DATA_PATH, num_img=10, latent_dim=latent_dim)\n",
        "mcp_save = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH/'e:{epoch:02d}',period=10, save_weights_only=True)\n",
        "\n",
        "model.fit(\n",
        "    train, epochs=epochs, callbacks=[save_images_callback, mcp_save], validation_data=valid\n",
        ")"
      ],
      "metadata": {
        "id": "z4Op5ie79OU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z67zpcrgyBDC"
      },
      "source": [
        "Some of the last generated images around epoch 30\n",
        "(results keep improving after that):\n",
        "\n",
        "![results](https://i.imgur.com/h5MtQZ7l.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "32_LvJVcZqcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Model selection\n",
        "use_gan = False\n",
        "\n",
        "load_weights=True\n",
        "weights_path=MODEL_PATH/'e:01'"
      ],
      "metadata": {
        "id": "pWGmyfGSCIdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "if model_name == Model_names.GAN:\n",
        "    model = get_GAN(latent_dim, load_weights, weights_path)\n",
        "else:\n",
        "    model = get_VAE(latent_dim, load_weights, weights_path)"
      ],
      "metadata": {
        "id": "pkDBjg_PZ--v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating images"
      ],
      "metadata": {
        "id": "65FPnXLZfSYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_samples = 100\n",
        "\n",
        "# # Generate samples\n",
        "# imgs = gan.generate_samples(n_samples)\n",
        "\n",
        "# # Initialize directory for pictures\n",
        "# dir_name = f'DCGAN_CelebA-generated'\n",
        "# img_path = ROOT_DATA_PATH/dir_name\n",
        "# img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# # Save images\n",
        "# for i, img in enumerate(imgs):\n",
        "#     img.save(img_path/f'generated_img_{i}.png')"
      ],
      "metadata": {
        "id": "lRcxXYZtbzxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving random images **TODO fix with test**"
      ],
      "metadata": {
        "id": "biMA_U42f7ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "train, _, test = get_dataset_partitions_tf(dataset, size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjBc-WQ1f7y3",
        "outputId": "9b762abf-5f9b-42d0-c797-9ffcb0fedc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_name = f'DCGAN_CelebA-real'\n",
        "\n",
        "real_img_path = ROOT_DATA_PATH/'DCGAN_CelebA-real'\n",
        "real_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "gen_img_path = ROOT_DATA_PATH/'DCGAN_CelebA-gen'\n",
        "gen_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "i = 0\n",
        "for image_batch in test.as_numpy_iterator():\n",
        "    for j in range(image_batch.shape[0]): # batch of 32\n",
        "        # Generating img + saving it\n",
        "        img_gen = gan.generate_samples(1)[0]\n",
        "        img_gen.save(gen_img_path/f'{i}.png')\n",
        "\n",
        "        # Saving real image\n",
        "        img_real = image_batch[j,...]\n",
        "        plt.imsave(real_img_path/f'{i}.png', img_real / 255)\n",
        "        i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "7HfX2OlfhHGu",
        "outputId": "3d4dd567-c44d-42c9-a104-72d2f31cc3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-d319110a1beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Generating img + saving it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimg_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_img_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{i}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9d5dd8728877>\u001b[0m in \u001b[0;36mgenerate_samples\u001b[0;34m(self, num_img)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FID"
      ],
      "metadata": {
        "id": "jMTi8OaIZ2Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yBK1xSWZscB",
        "outputId": "3892be38-fbb0-4e0c-d09b-8b8e0914299b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch-fid-0.2.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.21.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->pytorch-fid) (4.1.1)\n",
            "Building wheels for collected packages: pytorch-fid\n",
            "  Building wheel for pytorch-fid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fid: filename=pytorch_fid-0.2.1-py3-none-any.whl size=14835 sha256=5090abf0d28a2ecc5e60f0bebd08b90fda9e408801d191841ef0be954cba2519\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/ac/03/c5634775c8a64f702343ef5923278f8d3bb8c651debc4a6890\n",
            "Successfully built pytorch-fid\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid $real_img_path $gen_img_path --device cuda:0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p5-aPZEZwOw",
        "outputId": "da2dcfcc-aa3e-4a47-d3d8-daac0a9eb839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:05<00:00, 17.2MB/s]\n",
            "100% 719/719 [02:53<00:00,  4.14it/s]\n",
            "100% 333/333 [01:12<00:00,  4.60it/s]\n",
            "FID:  51.430461225018604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FID:  51.430461225018604"
      ],
      "metadata": {
        "id": "4_SJy8c9ADtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GEI8C9h-ZwRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specificity"
      ],
      "metadata": {
        "id": "CAlwv1T1Z3-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(A, B):\n",
        "    \"\"\"Works on Batches and single images\"\"\"\n",
        "    return tf.norm(reference_img-image_batch, axis(1,2,3))\n",
        "\n",
        "def find_closest_img(reference_img, dataset, dist_func=None):\n",
        "    \"\"\"\n",
        "    code below finds closest image from reference_img\n",
        "    takes 2 mins for a single img ...\n",
        "    \"\"\"\n",
        "    closest_dist = 1e9\n",
        "    closest_img = None\n",
        "    for image_batch in dataset.as_numpy_iterator():\n",
        "        \n",
        "        distances = euclidean_distance(reference_img, image_batch)\n",
        "        i = tf.math.argmin(distances).numpy()\n",
        "        min_dist = distances[i]\n",
        "\n",
        "        if closest_dist > min_dist:\n",
        "            closest_dist = min_dist\n",
        "            closest_img = truth[i]\n",
        "    return closest_img, closest_dist"
      ],
      "metadata": {
        "id": "oKJHxoE6Z5sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "min_dist = []\n",
        "n_samples = 10\n",
        "imgs = model.generate_samples(n_samples)\n",
        "for img in imgs:\n",
        "    closest_img, dist = find_closest_img(img, train)\n",
        "    min_dist.append(dist)\n",
        "print(np.mean(np.array(min_dist)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiqXMPgvhdWS",
        "outputId": "a9bb479e-944b-4580-d265-2798949df9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5223.487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5223.487"
      ],
      "metadata": {
        "id": "RWjXGDsJVzVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "1uCD5-iHhdpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb\n"
      ],
      "metadata": {
        "id": "7JJbZw1ghd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3e9TJNOhgar",
        "outputId": "d8764aae-1e0d-4f06-8601-fcf2dff5b89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8WHoN2O9hi-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pOeojm615FW9",
        "o1fAbtpW5PID",
        "JYsq8qxt7y0h"
      ],
      "name": "main",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}