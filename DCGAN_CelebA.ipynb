{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poclecoqq/INF8225-projet/blob/main/DCGAN_CelebA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaRrf9VjyBCw"
      },
      "source": [
        "# DCGAN to generate face images\n",
        "\n",
        "Adapted from [this tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCLBll_NyBC1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LCwF4rBVyBC2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def is_in_collab_env():\n",
        "    return 'google.colab' in sys.modules\n",
        "\n",
        "if is_in_collab_env():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT = Path(\"/content/drive/MyDrive/INF8225/notebooks\")\n",
        "else:\n",
        "    ROOT = Path('..').resolve()"
      ],
      "metadata": {
        "id": "hNPxBlC227lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415e4975-2668-46ed-c927-83bf45834a98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = ROOT/\"data\"\n",
        "MODEL_PATH = ROOT/\"models\"\n",
        "\n",
        "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "cUnqb5rI2-rE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPUbJUhByBC4"
      },
      "source": [
        "## Prepare CelebA data\n",
        "\n",
        "We'll use face images from the CelebA dataset, resized to 64x64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T9VZevZayBC4"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"celeba_gan\", exist_ok=True)\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwwmdFUEyBC5"
      },
      "source": [
        "Create a dataset from our folder, and rescale the images to the [0-1] range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-LIsh-79yBC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a198ef-0729-49ac-de51-8b18a78cf29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0zHMM3jyBC7"
      },
      "source": [
        "Let's display a sample image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9b1RqfjdyBC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "d599c1c4-e16e-4b3f-a98c-d90bf22bf128"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1lEcrl0yBC9"
      },
      "source": [
        "## Create the discriminator\n",
        "\n",
        "It maps a 64x64 image to a binary classification score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DoI5i_GqyBC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3fa13a-e497-469a-bdda-7d3102dac03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 404,801\n",
            "Trainable params: 404,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES7_eC6iyBC-"
      },
      "source": [
        "## Create the generator\n",
        "\n",
        "It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RBDiarp7yBC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efe780c-ea94-40bb-d91c-9bcc57bd7d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              1056768   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 128)      262272    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 256)      524544    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 3)         38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,979,651\n",
            "Trainable params: 3,979,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVzXNsaIyBC_"
      },
      "source": [
        "## Override `train_step`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D0p0yY5NyBC_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "    \n",
        "    def generate_samples(self, num_img):\n",
        "        \"\"\"\n",
        "        Generates num_img images.\n",
        "            Returns:\n",
        "                imgs: an array of PIL images\n",
        "        \"\"\"\n",
        "        random_latent_vectors = tf.random.normal(shape=(num_img, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imgs = [ keras.preprocessing.image.array_to_img(generated_image) for generated_image in generated_images]\n",
        "        return imgs\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uQVOMmHyBDB"
      },
      "source": [
        "## Create a callback that periodically saves generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ejwQuN9HyBDB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128, epoch_offset=0):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.epoch_offset = epoch_offset\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch_adjusted = self.epoch_offset+epoch\n",
        "        if (epoch_adjusted % 10) == 0:\n",
        "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "            generated_images = self.model.generator(random_latent_vectors)\n",
        "            generated_images *= 255\n",
        "            generated_images.numpy()\n",
        "            \n",
        "            dir_name = f'DCGAN_CelebA-e:{epoch_adjusted}'\n",
        "            img_path = DATA_PATH/dir_name\n",
        "            img_path.mkdir(parents=True, exist_ok=True)\n",
        "            crnt_timestamp = str(time.time()).split('.')[0]\n",
        "            for i in range(self.num_img):\n",
        "                img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "                img.save(img_path/f'generated_img_e:{epoch_adjusted}_{i}_t:{crnt_timestamp}.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeOQ2YkyBDC"
      },
      "source": [
        "## Train the end-to-end model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlIRBCi3yBDC"
      },
      "outputs": [],
      "source": [
        "# Code to start training from scratch\n",
        "# epochs = 100  # In practice, use ~100 epochs\n",
        "# gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "# gan.compile(\n",
        "#     d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "#     g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "#     loss_fn=keras.losses.BinaryCrossentropy(),\n",
        "# )\n",
        "# gan_monitor = GANMonitor(num_img=10, latent_dim=latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to continue training\n",
        "epochs = 30\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "gan.load_weights(MODEL_PATH/'DCGAN_CelebA-70+30') \n",
        "gan_monitor = GANMonitor(num_img=10, latent_dim=latent_dim, epoch_offset=100)"
      ],
      "metadata": {
        "id": "6T_F2v4gkW-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcp_save = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH/'DCGAN_CelebA-100+{epoch:02d}',period=10, save_weights_only=True)\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[gan_monitor, mcp_save]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Op5ie79OU4",
        "outputId": "18579ca6-7e7f-49f6-ef3b-74b7fc3a06fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/30\n",
            "6332/6332 [==============================] - 1911s 299ms/step - d_loss: 0.5252 - g_loss: 1.2979\n",
            "Epoch 2/30\n",
            "6332/6332 [==============================] - 1911s 302ms/step - d_loss: 0.5247 - g_loss: 1.3032\n",
            "Epoch 3/30\n",
            "6332/6332 [==============================] - 1915s 302ms/step - d_loss: 0.5222 - g_loss: 1.3029\n",
            "Epoch 4/30\n",
            "6332/6332 [==============================] - 1917s 303ms/step - d_loss: 0.5203 - g_loss: 1.3102\n",
            "Epoch 5/30\n",
            "6332/6332 [==============================] - 1916s 303ms/step - d_loss: 0.5189 - g_loss: 1.3229\n",
            "Epoch 6/30\n",
            "6332/6332 [==============================] - 1904s 301ms/step - d_loss: 0.5174 - g_loss: 1.3140\n",
            "Epoch 7/30\n",
            "6332/6332 [==============================] - 1904s 301ms/step - d_loss: 0.5177 - g_loss: 1.3222\n",
            "Epoch 8/30\n",
            "6332/6332 [==============================] - 1903s 301ms/step - d_loss: 0.5153 - g_loss: 1.3266\n",
            "Epoch 9/30\n",
            "6332/6332 [==============================] - 1907s 301ms/step - d_loss: 0.5133 - g_loss: 1.3386\n",
            "Epoch 10/30\n",
            "6332/6332 [==============================] - 1918s 303ms/step - d_loss: 0.5129 - g_loss: 1.3405\n",
            "Epoch 11/30\n",
            "6332/6332 [==============================] - 1918s 303ms/step - d_loss: 0.5095 - g_loss: 1.3543\n",
            "Epoch 12/30\n",
            "6332/6332 [==============================] - 1913s 302ms/step - d_loss: 0.5086 - g_loss: 1.3627\n",
            "Epoch 13/30\n",
            "6332/6332 [==============================] - 1901s 300ms/step - d_loss: 0.5069 - g_loss: 1.3531\n",
            "Epoch 14/30\n",
            "6332/6332 [==============================] - 1903s 301ms/step - d_loss: 0.5063 - g_loss: 1.3647\n",
            "Epoch 15/30\n",
            "6332/6332 [==============================] - 1901s 300ms/step - d_loss: 0.5040 - g_loss: 1.3767\n",
            "Epoch 16/30\n",
            "6332/6332 [==============================] - 1903s 300ms/step - d_loss: 0.5019 - g_loss: 1.3754\n",
            "Epoch 17/30\n",
            "6332/6332 [==============================] - 1902s 300ms/step - d_loss: 0.5008 - g_loss: 1.3850\n",
            "Epoch 18/30\n",
            "6332/6332 [==============================] - 1904s 301ms/step - d_loss: 0.5009 - g_loss: 1.3809\n",
            "Epoch 19/30\n",
            "6332/6332 [==============================] - 1914s 302ms/step - d_loss: 0.4983 - g_loss: 1.3865\n",
            "Epoch 20/30\n",
            "6332/6332 [==============================] - 1914s 302ms/step - d_loss: 0.4969 - g_loss: 1.4033\n",
            "Epoch 21/30\n",
            "6332/6332 [==============================] - 1912s 302ms/step - d_loss: 0.4941 - g_loss: 1.4029\n",
            "Epoch 22/30\n",
            "6332/6332 [==============================] - 1914s 302ms/step - d_loss: 0.4940 - g_loss: 1.4045\n",
            "Epoch 23/30\n",
            "6332/6332 [==============================] - 1912s 302ms/step - d_loss: 0.4908 - g_loss: 1.4103\n",
            "Epoch 24/30\n",
            "6332/6332 [==============================] - 1912s 302ms/step - d_loss: 0.4874 - g_loss: 1.4244\n",
            "Epoch 25/30\n",
            "6332/6332 [==============================] - 1904s 301ms/step - d_loss: 0.4873 - g_loss: 1.4334\n",
            "Epoch 26/30\n",
            "6332/6332 [==============================] - 1901s 300ms/step - d_loss: 0.4860 - g_loss: 1.4396\n",
            "Epoch 27/30\n",
            "6332/6332 [==============================] - 1900s 300ms/step - d_loss: 0.4861 - g_loss: 1.4368\n",
            "Epoch 28/30\n",
            "6332/6332 [==============================] - 1882s 297ms/step - d_loss: 0.4830 - g_loss: 1.4457\n",
            "Epoch 29/30\n",
            "6332/6332 [==============================] - 1867s 295ms/step - d_loss: 0.4806 - g_loss: 1.4608\n",
            "Epoch 30/30\n",
            "6332/6332 [==============================] - 1868s 295ms/step - d_loss: 0.4805 - g_loss: 1.4595\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6549f94dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z67zpcrgyBDC"
      },
      "source": [
        "Some of the last generated images around epoch 30\n",
        "(results keep improving after that):\n",
        "\n",
        "![results](https://i.imgur.com/h5MtQZ7l.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1hiO3kYLJvoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics\n"
      ],
      "metadata": {
        "id": "32_LvJVcZqcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model"
      ],
      "metadata": {
        "id": "uCa2nMLIfVzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "gan.load_weights(MODEL_PATH/'DCGAN_CelebA-100+30') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkDBjg_PZ--v",
        "outputId": "5a4b81be-ee11-40a5-e2f9-da10aa1a422d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f322d8e2f50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating images"
      ],
      "metadata": {
        "id": "65FPnXLZfSYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_samples = 100\n",
        "\n",
        "# # Generate samples\n",
        "# imgs = gan.generate_samples(n_samples)\n",
        "\n",
        "# # Initialize directory for pictures\n",
        "# dir_name = f'DCGAN_CelebA-generated'\n",
        "# img_path = DATA_PATH/dir_name\n",
        "# img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# # Save images\n",
        "# for i, img in enumerate(imgs):\n",
        "#     img.save(img_path/f'generated_img_{i}.png')"
      ],
      "metadata": {
        "id": "lRcxXYZtbzxp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving random images **TODO fix with test**"
      ],
      "metadata": {
        "id": "biMA_U42f7ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "train, _, test = get_dataset_partitions_tf(dataset, size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjBc-WQ1f7y3",
        "outputId": "9b762abf-5f9b-42d0-c797-9ffcb0fedc89"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_name = f'DCGAN_CelebA-real'\n",
        "\n",
        "real_img_path = DATA_PATH/'DCGAN_CelebA-real'\n",
        "real_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "gen_img_path = DATA_PATH/'DCGAN_CelebA-gen'\n",
        "gen_img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "i = 0\n",
        "for image_batch in test.as_numpy_iterator():\n",
        "    for j in range(image_batch.shape[0]): # batch of 32\n",
        "        # Generating img + saving it\n",
        "        img_gen = gan.generate_samples(1)[0]\n",
        "        img_gen.save(gen_img_path/f'{i}.png')\n",
        "\n",
        "        # Saving real image\n",
        "        img_real = image_batch[j,...]\n",
        "        plt.imsave(real_img_path/f'{i}.png', img_real / 255)\n",
        "        i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "7HfX2OlfhHGu",
        "outputId": "3d4dd567-c44d-42c9-a104-72d2f31cc3fe"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-d319110a1beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Generating img + saving it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimg_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_img_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{i}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9d5dd8728877>\u001b[0m in \u001b[0;36mgenerate_samples\u001b[0;34m(self, num_img)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FID"
      ],
      "metadata": {
        "id": "jMTi8OaIZ2Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yBK1xSWZscB",
        "outputId": "3892be38-fbb0-4e0c-d09b-8b8e0914299b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch-fid-0.2.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.21.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->pytorch-fid) (4.1.1)\n",
            "Building wheels for collected packages: pytorch-fid\n",
            "  Building wheel for pytorch-fid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fid: filename=pytorch_fid-0.2.1-py3-none-any.whl size=14835 sha256=5090abf0d28a2ecc5e60f0bebd08b90fda9e408801d191841ef0be954cba2519\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/ac/03/c5634775c8a64f702343ef5923278f8d3bb8c651debc4a6890\n",
            "Successfully built pytorch-fid\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid $real_img_path $gen_img_path --device cuda:0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p5-aPZEZwOw",
        "outputId": "da2dcfcc-aa3e-4a47-d3d8-daac0a9eb839"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:05<00:00, 17.2MB/s]\n",
            "100% 719/719 [02:53<00:00,  4.14it/s]\n",
            "100% 333/333 [01:12<00:00,  4.60it/s]\n",
            "FID:  51.430461225018604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FID:  51.430461225018604"
      ],
      "metadata": {
        "id": "4_SJy8c9ADtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GEI8C9h-ZwRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specificity"
      ],
      "metadata": {
        "id": "CAlwv1T1Z3-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_img(reference_img, dataset, dist_func=None):\n",
        "    \"\"\"\n",
        "    code below finds closest image from reference_img\n",
        "    takes 2 mins for a single img ...\n",
        "    \"\"\"\n",
        "    closest_dist = 1e9\n",
        "    closest_img = None\n",
        "    for image_batch in dataset.as_numpy_iterator():\n",
        "        \n",
        "        truth = image_batch\n",
        "        distances = [tf.norm(truth[i]-reference_img) for i in range(32)]\n",
        "        i = tf.math.argmin(distances).numpy()\n",
        "        min_dist = distances[i]\n",
        "\n",
        "        if closest_dist > min_dist:\n",
        "            closest_dist = min_dist\n",
        "            closest_img = truth[i]\n",
        "    return closest_img"
      ],
      "metadata": {
        "id": "oKJHxoE6Z5sZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "min_dist = []\n",
        "n_samples = 10\n",
        "imgs = gan.generate_samples(n_samples)\n",
        "for img in imgs:\n",
        "    closest_img = find_closest_img(img, train)\n",
        "    min_dist.append(tf.norm(img-closest_img))\n",
        "print(np.mean(np.array(min_dist)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiqXMPgvhdWS",
        "outputId": "a9bb479e-944b-4580-d265-2798949df9f2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5223.487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5223.487"
      ],
      "metadata": {
        "id": "RWjXGDsJVzVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "1uCD5-iHhdpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb\n"
      ],
      "metadata": {
        "id": "7JJbZw1ghd31"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3e9TJNOhgar",
        "outputId": "d8764aae-1e0d-4f06-8601-fcf2dff5b89b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8WHoN2O9hi-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oCLBll_NyBC1",
        "sPUbJUhByBC4",
        "p1lEcrl0yBC9",
        "ES7_eC6iyBC-",
        "XVzXNsaIyBC_"
      ],
      "name": "DCGAN_CelebA",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}